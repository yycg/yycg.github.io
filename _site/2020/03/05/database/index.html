<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
    <meta name="description" content="没有对生活绝望，就不会爱生活。——阿尔贝·加缪">
    <meta name="keywords"  content="">
    <meta name="theme-color" content="#000000">
    
    <!-- Open Graph -->
    <meta property="og:title" content="数据库 - 盈盈冲哥的博客">
    
    <meta property="og:type" content="article">
    <meta property="og:description" content="MySQL
">
    
    <meta property="article:published_time" content="2020-03-05T20:00:00Z">
    
    
    <meta property="article:author" content="盈盈冲哥">
    
    
    <meta property="article:tag" content="学习">
    
    
    <meta property="og:image" content="http://localhost:4000/img/a-perfect-world.jpg">
    <meta property="og:url" content="http://localhost:4000/2020/03/05/database/">
    <meta property="og:site_name" content="盈盈冲哥的博客">
    
    <title>数据库 - 盈盈冲哥的博客</title>

    <!-- Web App Manifest -->
    <link rel="manifest" href="/pwa/manifest.json">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/img/letter-y.ico">
    
    <!-- Canonical URL -->
    <link rel="canonical" href="http://localhost:4000/2020/03/05/database/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.min.css">

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->

<nav class="navbar navbar-default navbar-custom navbar-fixed-top">

    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">盈盈冲哥的博客</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>
                    
                    
                    
                    
                    <li>
                        <a href="/about/">About</a>
                    </li>
                    
                    
                    
                    <li>
                        <a href="/archive/">Archive</a>
                    </li>
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    var __HuxNav__ = {
        close: function(){
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        },
        open: function(){
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }

    // Bind Event
    $toggle.addEventListener('click', function(e){
        if ($navbar.className.indexOf('in') > 0) {
            __HuxNav__.close()
        }else{
            __HuxNav__.open()
        }
    })

    /**
     * Since Fastclick is used to delegate 'touchstart' globally
     * to hack 300ms delay in iOS by performing a fake 'click',
     * Using 'e.stopPropagation' to stop 'touchstart' event from 
     * $toggle/$collapse will break global delegation.
     * 
     * Instead, we use a 'e.target' filter to prevent handler
     * added to document close HuxNav.  
     *
     * Also, we use 'click' instead of 'touchstart' as compromise
     */
    document.addEventListener('click', function(e){
        if(e.target == $toggle) return;
        if(e.target.className == 'icon-bar') return;
        __HuxNav__.close();
    })
</script>


    <!-- Image to hack wechat -->
<!-- <img src="/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="/img/fleabag.jpg" width="0" height="0"> -->

<!-- Post Header -->



<style type="text/css">
    header.intro-header{
        position: relative;
        background-image: url('/img/fleabag.jpg');
        background: ;
    }

    
</style>

<header class="intro-header" >

    <div class="header-mask"></div>
    
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/archive/?tag=%E5%AD%A6%E4%B9%A0" title="学习">学习</a>
                        
                    </div>
                    <h1>数据库</h1>
                    
                    <h2 class="subheading"></h2>
                    <span class="meta">Posted by 盈盈冲哥 on March 5, 2020</span>
                </div>
            </div>
        </div>
    </div>
</header>






<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <!-- Multi-Lingual -->
                

				<h4 id="mysql">MySQL</h4>

<blockquote>
  <p>高性能MySQL</p>
</blockquote>

<ul>
  <li>
    <p>可以使用B-Tree索引的查询类型</p>

    <p>key(last_name, first_name, dob)</p>

    <p>前面所述的索引对如下类型的查询有效：</p>

    <ul>
      <li>全值匹配：全值匹配指的是和索引中的所有列进行匹配，例如前面提到的索引可用于查找姓名为Cuba Allen、出生于1960-01-01的人。</li>
      <li>匹配最左前缀：前面提到的索引可用于查找所有姓为Allen的人，即只使用索引的第一列。</li>
      <li>匹配列前缀：也可以只匹配某一列的值的开头部分。例如前面提到的索引可用于查找所有以J开头的姓的人。</li>
      <li>匹配范围值：例如前面提到的索引可用于查找姓在Allen和Barrymore之间的人。这里也只使用了索引的第一列。</li>
      <li>精确匹配某一列并范围匹配另外一列：前面提到的索引也可用于查找所有姓为Allen，并且名字是字母K开头（比如Kim、Karl等）的人。即第一列last_name全匹配，第二列first_name范围匹配。</li>
      <li>只访问索引的查询：B-Tree通常可以支持“只访问索引的查询”，即查询只需要访问索引，而无需访问数据行。后面我们将单独讨论这种“覆盖索引”的优化。</li>
    </ul>

    <p>下面是一些关于B-Tree索引的限制：</p>

    <ul>
      <li>如果不是按照索引的最左列开始查找，则无法使用索引。例如上面例子中的索引无法用于查找名字为Bill的人，也无法查找某个特定生日的人，因为这两列都不是最左数据列。类似地，也无法查找姓氏以某个字母结尾的人。</li>
      <li>不能跳过索引中的列。也就是说，前面所述的索引无法用于查找姓为Smith并且某个特定日期出生的人。如果不指定名 (first_name)，则MySQL只能使用索引的第一列。</li>
      <li>如果查询中有某个列的查询范围，则其右边所有列都无法使用索引优化查询。例如有查询WHERE last_name=’Smith’ AND first_name LIKE ‘J%’ AND dob = ‘1976-12-23’，这个查询只能使用索引的前两列，因为这里LIKE是一个范围条件（但是服务器可以把其余列用于其他目的）。如果范围查询列值的数量有限，那么可以通过使用多个等于条件来代替范围条件。</li>
    </ul>
  </li>
  <li>
    <p>高性能的索引策略</p>

    <ul>
      <li>
        <p>独立的列：独立的列是指<strong>索引列不能是表达式的一部分，也不能是函数的参数</strong>。</p>
      </li>
      <li>
        <p>前缀索引和索引选择性</p>

        <p>有时候需要索引很长的字符列，这会让索引变得大且慢。</p>

        <p>通常可以<strong>索引开始的部分字符</strong>，这样可以大大节约索引空间，从而提高索引效率。但这样也会降低索引的选择性。索引的选择性是指，不重复的索引值（也称为基数，cardinality）和数据表的记录总数（#T）的比值，范围从1/#T到1之间。索引的选择性越高则查询效率越高，因为选择性高的索引可以让MySQL在查找时过滤掉更多的行。唯一索引的选择性是1，这是最好的索引选择性，性能也是最好的。</p>

        <p>一般情况下某个列前缀的选择性也是足够高的，足以满足查询性能。对于BLOB、TEXT或者很长的VARCHAR类型的列，必须使用前缀索引，因为MySQL不允许索引这些列的完整长度。</p>

        <p>诀窍在于要选择足够长的前缀以保证较高的选择性，同时又不能太长（以便节约空间）。前缀应该足够长，以使得前缀索引的选择性接近于索引整个列。换句话说，前缀的“基数”应该接近于完整列的“基数”。</p>
      </li>
      <li>
        <p>多列索引</p>
      </li>
      <li>
        <p><strong>选择合适的索引列顺序</strong></p>

        <p>在一个多列B-Tree索引中，索引列的顺序意味着索引首先按照最左列进行排序，其次是第二列，等等。所以，索引可以按照升序或者降序进行扫描，以满足精确符合列顺序的ORDER BY、GROUP BY和DISTINCT等子句的查询需求。所以多列索引的列顺序至关重要。</p>

        <p>对于如何选择索引的列顺序有一个经验法则：<strong>将选择性最高的列放到索引的最前列</strong>。当不需要开率排序和分组时，将选择性最高的列放在前面通常时很好的。这时候索引确实能够最快地过滤出需要的行，对于在WHERE子句中只使用了索引部分前缀列的查询来说选择性也更高。然而，性能不只是依赖于所有索引列的选择性，也和查询条件的具体值有关，也就是和值的分布有关。这和前面介绍的选择前缀的长度需要考虑的地方一样。可能需要根据那些<strong>运行频率最高</strong>的查询来调整索引列的顺序，让这种情况下索引的选择性最高。</p>
      </li>
      <li>
        <p>聚簇索引</p>

        <p><strong>聚簇索引并不是一种单独的索引类型，而是一种数据存储方式。</strong>InnoDB的聚簇索引实际上在同一个结构种保存了B-Tree索引和数据行。</p>

        <p>当表有聚簇索引时，它的数据行实际上存放在索引的叶子页中。属于“聚簇”表示数据行和相邻的键值紧凑地存储在一起。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。</p>

        <p>聚簇索引的优点：</p>

        <ul>
          <li>可以把相关数据保存在一起。例如实现电子邮件时，可以根据用户ID来聚集数据，这样只需要从磁盘读取少数的数据页就能获取某个用户的全部邮件。如果没有使用聚簇索引，则每封邮件都可能导致一次磁盘I/O。</li>
          <li>数据访问更快。聚簇索引将索引和数据保存在同一个B-Tree中，因此从聚簇索引中获取数据通常比在非聚簇索引中查找要快。</li>
          <li>使用覆盖索引扫描的查询可以直接使用页节点中的主键值。</li>
        </ul>

        <p>聚簇索引的缺点：</p>

        <ul>
          <li>…</li>
          <li>
            <p>二级索引（非聚簇索引）访问需要两次索引查找，而不是一次。</p>

            <p>为什么二级索引需要两次索引查找？答案在于二级索引中保存的“行指针”的实质。要记住，二级索引叶子节点保存的不是指向行的物理位置的指针，而是行的主键值。</p>

            <p>这意味这通过二级索引查找行，存储殷勤需要找到二级索引的叶子节点获得相应的主键值，然后根据这个值去聚簇索引中查找到对应的行。这里做了重复的工作：两次B-Tree查找而不是一次。</p>
          </li>
        </ul>

        <p>InnoDB和MyISAM的数据分布对比</p>

        <ul>
          <li>
            <p>MyISAM的数据分布</p>

            <p>MyISAM按照数据插入的顺序存储在磁盘上。在行的旁边显示了行号，从0开始递增。</p>

            <p>MyISAM中主键索引和其他索引在结构上没有什么不同。主键索引就是一个名为PRIMARY的唯一非空索引。</p>
          </li>
          <li>
            <p>InnoDB的数据分布</p>

            <p>在InnoDB中，聚簇索引“就是”表，所以不想MyISAM那样需要独立的行存储。</p>

            <p>聚簇索引的每一个叶子节点都包含了主键值、事务ID、用于事务和MVCC的回滚指针以及所有的剩余列。</p>

            <p>还有一点和MyISAM的不同是，InnoDB的二级索引和聚簇索引很不相同。InnoDB二级索引的叶子节点中存储的不是“行指针”，而是主键值，并以此作为指向行的“指针”。</p>
          </li>
        </ul>

        <p>在InnoDB表中按主键顺序插入行</p>

        <p>如果正在使用InnoDB表并且没有什么数据需要聚集，那么可以定义一个代理键作为主键，这种主键的数据应该和应用无关，最简单的方法是使用AUTO_INCREMENT自增列。这样可以保证数据行是按顺序写入，对于根据主键做关联操作的性能也会更好。</p>

        <p>最好避免随机的（不连续且值的分布范围非常大）聚簇索引，特别时对于I/O密集型的应用。例如，从性能的角度，使用UUID来作为聚簇索引则会很糟糕：它使得聚簇索引的插入变得完全随机，这是最坏的情况，使得数据没有任何聚集特性。</p>

        <p>注意到向UUID主键插入行不仅花费的时间更长，而且索引占用的空间页更大。这一方面是由于主键字段更长，另一方面毫无疑问是由于页分裂和碎片导致的。</p>

        <p>因为主键的值是顺序的，所以InnoDB把每一条记录都存储在上一条记录的后面。当达到页的最大填充因此时（InnoDB默认的最大填充因子时页大小的15/16，留出部分空间用于以后修改），下一条记录就会写入新的页中。一旦数据按照这种顺序的方式加载，主键页就会近似于被顺序的记录填满，这也正是所期望的结果。</p>

        <p>对比一下向第二个使用了UUID聚簇索引的表插入数据，看看有什么不同。因为新行的主键值不一定比之前插入的大，所以InnoDB无法简单地总是把新行插入到索引的最后，而是需要为新的行寻找合适的位置，并导致数据分布不够优化。</p>
      </li>
      <li>
        <p><strong>覆盖索引</strong></p>

        <p>如果索引的叶子节点中已经包含要查询的数据，那么还有什么必要再回表查询呢？如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为“覆盖索引”。</p>

        <p>当发区一个被索引覆盖的查询（也叫做索引覆盖查询）时，在EXPLAIN的EXTRA列可以看到Using index的信息。</p>
      </li>
      <li>
        <p><strong>使用索引扫描来做排序</strong></p>

        <p>MySQL有两种方式可以生成有序的结果：通过排序操作，或者按索引顺序扫描，如果EXPLAIN出来的type列的值为index，则说明mySQL使用了索引扫描来做排序。</p>

        <p>扫描索引本身是很快的，因为只需要从一条索引记录移动到紧接着的下一条记录。但如果索引不能覆盖查询所需的全部列，那就不得不每扫描一条索引记录就都回表查询一次对应的行。这基本上都是随机I/O，因此按索引顺序读取数据的速度通常要比顺序地全表扫描慢，尤其是在I/O密集型的工作负载时。</p>

        <p>只有当索引的列顺序和ORDER BY子句的顺序完全一致，并且所有列的排序方向（倒序或正序）都一样时，MySQL才能够使用索引来对结果做排序。如果查询需要关联多张表，则只有当ORDER BY子句引用的字段全部为第一个表时，才能使用索引做排序。ORDER BY子句和查找型查询的限制是一样的：需要满足索引的最左前缀的要求；否则，MySQL都需要执行排序操作，而无法利用索引排序。</p>

        <p>UNIQUE KEY rental_date (rental_date, inventory_id, customer_id)</p>

        <p>下面这个查询没有问题，因为ORDER BY使用的两列就是索引的最左前缀：</p>

        <p>… WHERE rental_date &gt; ‘2005-05-25’ ORDER BY rental_date, inventory_id;</p>
      </li>
    </ul>
  </li>
  <li>
    <p>查询性能优化</p>

    <ul>
      <li>
        <p>慢查询基础：优化数据访问</p>

        <ol>
          <li><strong>是否向数据库请求了不需要的数据：查询不需要的记录(LIMIT)、多表关联时返回全部列、总是取出全部列(SELECT *)、重复查询相同的数据（缓存）</strong></li>
          <li>MySQL是否在扫描额外的记录</li>
        </ol>
      </li>
      <li>
        <p>重构查询的方式</p>

        <ul>
          <li>一个复杂的查询还是多个简单的查询</li>
          <li>
            <p>切分查询</p>

            <p>有时候对于一个大查询我们需要“分而治之”，将大查询切分成小查询，每个查询功能完全一样，只完成一小部分，每次只返回一小部分查询结果。</p>

            <p>删除旧的数据就是一个很好的例子。定期地清除大量数据时，如果用一个大的语句一次性完成的话，则可能需要一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。将一个大的DELETE语句切分成多个较小的查询可以尽可能小地影响MySQL性能，同时还可以减少MySQL复制的延迟。</p>
          </li>
          <li>分解关联关系</li>
        </ul>
      </li>
      <li>
        <p>查询执行的基础</p>

        <p>当向MySQL发送一个请求的时候，MySQL到底做了些什么：</p>

        <ol>
          <li>客户端发送一条查询给服务器。</li>
          <li>服务端先检查查询缓存，如果命中了缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段。</li>
          <li>
            <p>服务器端进行SQL解析、预处理，再由优化器生成对应的执行计划。</p>

            <ul>
              <li>
                <p><strong>查询优化器</strong></p>

                <p>下面是一些MySQL能够处理的优化类型：</p>

                <ul>
                  <li>重新定义关联表的顺序</li>
                  <li>将外连接转化成内连接</li>
                  <li>使用等价变换规则</li>
                  <li><strong>优化COUNT()、MIN()和MAX()</strong>：要找到某一列的最小值，只需要查询对应B-Tree索引最左端的记录，MySQL可以直接获取索引的第一行记录。在优化器生成执行计划的时候就可以利用这一点，在B-Tree索引中，优化器会将这个表达式作为一个常数对待。类似的，如果要查找一个最大值，也只需要读取B-Tree索引的最后一条记录。类似的，没有任何WHERE条件的COUNT(*)查询通常也可以使用存储引擎提供的一些优化（例如，MyISAM维护了一个变量来存放数据表的行数）。</li>
                  <li>预估并转化为常数表达式：当MySQL检测到一个表达式可以转化为常数时，就会一直把该表达式作为常数进行优化处理。</li>
                  <li><strong>覆盖索引扫描</strong>：当索引中的列包含所有查询中需要使用的列的时候，MySQL就可以使用索引返回需要的数据，而无需查询对应的数据行。</li>
                  <li>子查询优化</li>
                  <li>提前终止查询</li>
                  <li>等值传播</li>
                  <li>列表IN()的比较：二分查找O(log n)，OR查询O(n)</li>
                </ul>
              </li>
              <li>关联查询优化器</li>
              <li>排序优化</li>
            </ul>
          </li>
          <li>MySQL根据优化器生成的执行计划，调用存储引擎的API来执行查询。</li>
          <li>将结果返回给客户端。</li>
        </ol>
      </li>
    </ul>
  </li>
</ul>

<hr />

<ul>
  <li>
    <p>内连接、左外连接、右外连接、全连接</p>

    <blockquote>
      <p><a href="https://blog.csdn.net/plg17/article/details/78758593">https://blog.csdn.net/plg17/article/details/78758593</a></p>
    </blockquote>
  </li>
  <li>
    <p>MySQL 七大约束</p>

    <blockquote>
      <p><a href="https://blog.csdn.net/apt1203jn/article/details/80314605">https://blog.csdn.net/apt1203jn/article/details/80314605</a></p>
    </blockquote>

    <ul>
      <li><strong>not null</strong> 不允许为空</li>
      <li><strong>default</strong> 默认值</li>
      <li>comment 列描述</li>
      <li>zerofill 在数据前面自动填充0</li>
      <li><strong>primary key</strong> 主键不能为空，不能重复，一张表有且只能有一个主键，可以是复合主键</li>
      <li><strong>auto_increment</strong> 自增长</li>
      <li><strong>unique</strong> 唯一键允许为空，但是不能重复</li>
    </ul>
  </li>
  <li>
    <p>MyISAM和InnoDB区别</p>

    <p>MyISAM是MySQL的默认数据库引擎（5.5版之前）。虽然性能极佳，而且提供了大量的特性，包括全文索引、压缩、空间函数等，但MyISAM不支持事务和行级锁，而且最大的缺陷就是崩溃后无法安全恢复。不过，5.5版本之后，MySQL引入了InnoDB（事务性数据库引擎），MySQL 5.5版本后默认的存储引擎为InnoDB。</p>

    <p>大多数时候我们使用的都是 InnoDB 存储引擎，但是在某些情况下使用 MyISAM 也是合适的比如读密集的情况下。（如果你不介意 MyISAM 崩溃恢复问题的话）。</p>

    <p>两者的对比：</p>

    <ol>
      <li><strong>是否支持行级锁</strong> : MyISAM 只有表级锁(table-level locking)，而InnoDB 支持行级锁(row-level locking)和表级锁,默认为行级锁。</li>
      <li><strong>是否支持事务和崩溃后的安全恢复</strong>： MyISAM 强调的是性能，每次查询具有原子性,其执行速度比InnoDB类型更快，但是不提供事务支持。但是InnoDB 提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。</li>
      <li><strong>是否支持外键</strong>： MyISAM不支持，而InnoDB支持。</li>
      <li><strong>是否支持MVCC</strong> ：仅 InnoDB 支持。应对高并发事务, MVCC比单纯的加锁更高效;MVCC只在 READ COMMITTED 和 REPEATABLE READ 两个隔离级别下工作;MVCC可以使用 乐观(optimistic)锁 和 悲观(pessimistic)锁来实现;各数据库中MVCC实现并不统一。推荐阅读：MySQL-InnoDB-MVCC多版本并发控制</li>
      <li>……</li>
    </ol>

    <p>《MySQL高性能》上面有一句话这样写到:</p>

    <blockquote>
      <p>不要轻易相信“MyISAM比InnoDB快”之类的经验之谈，这个结论往往不是绝对的。在很多我们已知场景中，InnoDB的速度都可以让MyISAM望尘莫及，尤其是用到了聚簇索引，或者需要访问的数据都可以放入内存的应用。</p>
    </blockquote>

    <p>一般情况下我们选择 InnoDB 都是没有问题的，但是某些情况下你并不在乎可扩展能力和并发能力，也不需要事务支持，也不在乎崩溃后的安全恢复问题的话，选择MyISAM也是一个不错的选择。但是一般情况下，我们都是需要考虑到这些问题的。</p>
  </li>
  <li>
    <p>字符集及校对规则</p>

    <p>字符集指的是一种从二进制编码到某类字符符号的映射。校对规则则是指某种字符集下的排序规则。MySQL中每一种字符集都会对应一系列的校对规则。</p>

    <p>MySQL采用的是类似继承的方式指定字符集的默认值，每个数据库以及每张数据表都有自己的默认值，他们逐层继承。比如：某个库中所有表的默认字符集将是该数据库所指定的字符集（这些表在没有指定字符集的情况下，才会采用默认字符集）</p>
  </li>
  <li>
    <p>索引</p>

    <p>MySQL索引使用的数据结构主要有<strong>BTree索引</strong> 和 <strong>哈希索引</strong> 。对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引。</p>

    <p>MySQL的BTree索引使用的是B树中的B+Tree，但对于主要的两种存储引擎的实现方式是不同的。</p>

    <ul>
      <li>
        <p><strong>MyISAM</strong>: B+Tree叶节点的data域存放的是数据记录的地址。在索引检索的时候，首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则<strong>取出其 data 域的值，然后以 data 域的值为地址读取相应的数据记录</strong>。这被称为“<strong>非聚簇索引</strong>”。</p>
      </li>
      <li>
        <p><strong>InnoDB</strong>: 其数据文件本身就是索引文件。相比MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按B+Tree组织的一个索引结构，<strong>树的叶节点data域保存了完整的数据记录</strong>。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。这被称为“<strong>聚簇索引（或聚集索引）</strong>”。而其余的索引都作为辅助索引，<strong>辅助索引的data域存储相应记录主键的值而不是地址</strong>，这也是和MyISAM不同的地方。<strong>在根据主索引搜索时，直接找到key所在的节点即可取出数据；在根据辅助索引查找时，则需要先取出主键的值，再走一遍主索引。 因此，在设计表的时候，不建议使用过长的字段作为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂。</strong></p>
      </li>
    </ul>

    <blockquote>
      <p><a href="https://www.kancloud.cn/kancloud/theory-of-mysql-index/41852">https://www.kancloud.cn/kancloud/theory-of-mysql-index/41852</a></p>
    </blockquote>
  </li>
  <li>
    <p>查询缓存的使用</p>

    <blockquote>
      <p>执行查询语句的时候，会先查询缓存。不过，MySQL 8.0 版本后移除，因为这个功能不太实用。</p>
    </blockquote>

    <p>my.cnf加入以下配置，重启MySQL开启查询缓存</p>

    <div class="language-properties highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="py">query_cache_type</span><span class="p">=</span><span class="s">1</span>
<span class="py">query_cache_size</span><span class="p">=</span><span class="s">600000</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>
    <p>MySQL执行以下命令也可以开启查询缓存</p>

    <div class="language-properties highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="err">set</span> <span class="err">global</span>  <span class="py">query_cache_type</span><span class="p">=</span><span class="s">1;</span>
<span class="err">set</span> <span class="err">global</span>  <span class="py">query_cache_size</span><span class="p">=</span><span class="s">600000;</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>
    <p>如上，<strong>开启查询缓存后在同样的查询条件以及数据情况下，会直接在缓存中返回结果。</strong> 这里的查询条件包括查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息。因此任何两个查询在任何字符上的不同都会导致缓存不命中。此外，如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、MySQL库中的系统表，其查询结果也不会被缓存。</p>

    <p>缓存建立之后，MySQL的查询缓存系统会跟踪查询中涉及的每张表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效。</p>

    <p><strong>缓存虽然能够提升数据库的查询性能，但是缓存同时也带来了额外的开销，每次查询后都要做一次缓存操作，失效后还要销毁。</strong> 因此，开启缓存查询要谨慎，尤其对于写密集的应用来说更是如此。如果开启，要注意合理控制缓存空间大小，一般来说其大小设置为几十MB比较合适。此外，<strong>还可以通过sql_cache和sql_no_cache来控制某个查询语句是否需要缓存：</strong></p>

    <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="k">select</span> <span class="n">sql_no_cache</span> <span class="k">count</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">from</span> <span class="n">usr</span><span class="p">;</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>
  </li>
  <li>
    <p>什么是事务?</p>

    <p><strong>事务是逻辑上的一组操作，要么都执行，要么都不执行。</strong></p>

    <p>事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：将小明的余额减少1000元，将小红的余额增加1000元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。</p>

    <p>事务的四大特性(ACID)</p>

    <ol>
      <li><strong>原子性（Atomicity）</strong>： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；</li>
      <li><strong>一致性（Consistency）</strong>： 执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的；</li>
      <li><strong>隔离性（Isolation）</strong>： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；</li>
      <li><strong>持久性（Durability）</strong>： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。</li>
    </ol>

    <blockquote>
      <p><a href="https://blog.csdn.net/chosen0ne/article/details/10036775">https://blog.csdn.net/chosen0ne/article/details/10036775</a></p>
    </blockquote>

    <p><strong>一致性（Consistency）</strong></p>

    <p>一致性是指事务使得系统从一个一致的状态转换到另一个一致状态。事务的一致性决定了一个系统设计和实现的复杂度。事务可以不同程度的一致性：</p>

    <ul>
      <li>
        <p>强一致性：读操作可以立即读到提交的更新操作。</p>
      </li>
      <li>
        <p>弱一致性：提交的更新操作，不一定立即会被读操作读到，此种情况会存在一个不一致窗口，指的是读操作可以读到最新值的一段时间。</p>
      </li>
      <li>
        <p>最终一致性：是弱一致性的特例。事务更新一份数据，最终一致性保证在没有其他事务更新同样的值的话，最终所有的事务都会读到之前事务更新的最新值。如果没有错误发生，不一致窗口的大小依赖于：通信延迟，系统负载等。</p>
      </li>
    </ul>

    <blockquote>
      <p><a href="https://www.zhihu.com/question/31346392">https://www.zhihu.com/question/31346392</a></p>
    </blockquote>

    <p>数据库一致性</p>

    <p>ACID里的AID都是数据库的特征,也就是依赖数据库的具体实现.而唯独这个C,实际上它依赖于应用层,也就是依赖于开发者.这里的一致性是指系统从一个正确的状态,迁移到另一个正确的状态.什么叫正确的状态呢?就是当前的状态满足预定的约束就叫做正确的状态.而事务具备ACID里C的特性是说通过事务的AID来保证我们的一致性.</p>

    <p>例子：A要向B支付100元,而A的账户中只有90元,并且我们给定账户余额这一列的约束是,不能小于0.那么很明显这条事务执行会失败,因为90-100=-10,小于我们给定的约束了.</p>
  </li>
  <li>
    <p><strong>并发事务带来哪些问题?</strong></p>

    <p>在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对同一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。</p>

    <ul>
      <li><strong>脏读（Dirty read）</strong>: 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。</li>
      <li><strong>丢失修改（Lost to modify）</strong>: 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。</li>
      <li><strong>不可重复读（Unrepeatableread）</strong>: 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。</li>
      <li><strong>幻读（Phantom read）</strong>: 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。</li>
    </ul>

    <p><strong>不可重复读和幻读区别：</strong></p>

    <p>不可重复读的重点是修改比如多次读取一条记录发现其中某些列的值被修改，幻读的重点在于新增或者删除比如多次读取一条记录发现记录增多或减少了。</p>
  </li>
  <li>
    <p><strong>事务隔离级别有哪些?MySQL的默认隔离级别是?</strong></p>

    <p>SQL 标准定义了四个隔离级别：</p>

    <ul>
      <li>READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，<strong>允许读取尚未提交的数据变更</strong>，可能会导致脏读、幻读或不可重复读。</li>
      <li>READ-COMMITTED(读取已提交)： <strong>允许读取并发事务已经提交的数据</strong>，可以阻止脏读，但是幻读或不可重复读仍有可能发生。</li>
      <li>REPEATABLE-READ(可重复读)： <strong>对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改</strong>，可以阻止脏读和不可重复读，但幻读仍有可能发生。</li>
      <li>SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。<strong>所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰</strong>，也就是说，该级别可以防止脏读、不可重复读以及幻读。</li>
    </ul>

    <table>
      <thead>
        <tr>
          <th>隔离级别</th>
          <th>脏读</th>
          <th>不可重复读</th>
          <th>幻影读</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>读取未提交(READ-UNCOMMITTED)</td>
          <td>√</td>
          <td>√</td>
          <td>√</td>
        </tr>
        <tr>
          <td>读取已提交(READ-COMMITTED)</td>
          <td>×</td>
          <td>√</td>
          <td>√</td>
        </tr>
        <tr>
          <td>可重复读(REPEATABLE-READ)</td>
          <td>×</td>
          <td>×</td>
          <td>√</td>
        </tr>
        <tr>
          <td>可串行化(SERIALIZABLE)</td>
          <td>×</td>
          <td>×</td>
          <td>×</td>
        </tr>
      </tbody>
    </table>

    <p>MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）。</p>

    <p>这里需要注意的是：与 SQL 标准不同的地方在于 InnoDB 存储引擎在 REPEATABLE-READ（可重读） 事务隔离级别下使用的是Next-Key Lock 锁算法，因此可以避免幻读的产生，这与其他数据库系统(如 SQL Server) 是不同的。所以说InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读） 已经可以完全保证事务的隔离性要求，即达到了 SQL标准的 SERIALIZABLE(可串行化) 隔离级别。因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是 READ-COMMITTED(读取提交内容) ，但是你要知道的是InnoDB 存储引擎默认使用 REPEATABLE-READ（可重读） 并不会有任何性能损失。</p>

    <p>InnoDB 存储引擎在 分布式事务 的情况下一般会用到 SERIALIZABLE(可串行化) 隔离级别。</p>
  </li>
  <li>
    <p><strong>锁机制与InnoDB锁算法</strong></p>

    <p>MyISAM和InnoDB存储引擎使用的锁：</p>

    <ul>
      <li>MyISAM采用表级锁(table-level locking)。</li>
      <li>InnoDB支持行级锁(row-level locking)和表级锁，默认为行级锁</li>
    </ul>

    <p>表级锁和行级锁对比：</p>

    <ul>
      <li>表级锁： MySQL中锁定 粒度最大 的一种锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率最高，并发度最低，MyISAM和 InnoDB引擎都支持表级锁。</li>
      <li>行级锁： MySQL中锁定 粒度最小 的一种锁，只针对当前操作的行进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。</li>
    </ul>

    <p>InnoDB存储引擎的锁的算法有三种：</p>

    <ul>
      <li>Record lock：单个行记录上的锁</li>
      <li>Gap lock：间隙锁，锁定一个范围，不包括记录本身</li>
      <li>Next-key lock：record+gap 锁定一个范围，包含记录本身</li>
    </ul>

    <p>相关知识点：</p>

    <ol>
      <li>innodb对于行的查询使用next-key lock</li>
      <li>Next-locking keying为了解决Phantom Problem幻读问题</li>
      <li>当查询的索引含有唯一属性时，将next-key lock降级为record key</li>
      <li>Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生</li>
      <li>有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） A. 将事务隔离级别设置为RC B. 将参数innodb_locks_unsafe_for_binlog设置为1</li>
    </ol>
  </li>
  <li>
    <p>大表优化</p>

    <p>当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下：</p>

    <ol>
      <li>
        <p>限定数据的范围</p>

        <p>务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内；</p>
      </li>
      <li>
        <p>读/写分离</p>

        <p>经典的数据库拆分方案，主库负责写，从库负责读；</p>
      </li>
      <li>
        <p>垂直分区</p>

        <p>根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。</p>

        <p>简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。 如下图所示，这样来说大家应该就更容易理解了。</p>

        <p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9E%82%E7%9B%B4%E5%88%86%E5%8C%BA.png" alt="img" /></p>

        <ul>
          <li>
            <p>垂直拆分的优点： 可以使得列数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。</p>
          </li>
          <li>
            <p>垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂；</p>
          </li>
        </ul>
      </li>
      <li>
        <p>水平分区</p>

        <p>保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。</p>

        <p>水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。</p>

        <p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%B0%B4%E5%B9%B3%E6%8B%86%E5%88%86.png" alt="img" /></p>

        <p>水平拆分可以支持非常大的数据量。需要注意的一点是：分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 水平拆分最好分库 。</p>

        <p>水平拆分能够 支持非常大的数据量存储，应用端改造也少，但 分片事务难以解决 ，跨节点Join性能较差，逻辑复杂。《Java工程师修炼之道》的作者推荐 尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度 ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络I/O。</p>
      </li>
    </ol>

    <p>下面补充一下数据库分片的两种常见方案：</p>

    <ul>
      <li>客户端代理： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 Sharding-JDBC 、阿里的TDDL是两种比较常用的实现。</li>
      <li>中间件代理： 在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。 我们现在谈的 Mycat 、360的Atlas、网易的DDB等等都是这种架构的实现。</li>
    </ul>
  </li>
  <li>
    <p>解释一下什么是池化设计思想。什么是数据库连接池?为什么需要数据库连接池?</p>

    <p>池化设计应该不是一个新名词。我们常见的如java线程池、jdbc连接池、redis连接池等就是这类设计的代表实现。这种设计会初始预设资源，解决的问题就是抵消每次获取资源的消耗，如创建线程的开销，获取远程连接的开销等。就好比你去食堂打饭，打饭的大妈会先把饭盛好几份放那里，你来了就直接拿着饭盒加菜即可，不用再临时又盛饭又打菜，效率就高了。除了初始化资源，池化设计还包括如下这些特征：池子的初始值、池子的活跃值、池子的最大值等，这些特征可以直接映射到java线程池和数据库连接池的成员属性中。</p>

    <p>数据库连接本质就是一个 socket 的连接。数据库服务端还要维护一些缓存和用户权限信息之类的 所以占用了一些内存。我们可以把数据库连接池是看做是维护的数据库连接的缓存，以便将来需要对数据库的请求时可以重用这些连接。为每个用户打开和维护数据库连接，尤其是对动态数据库驱动的网站应用程序的请求，既昂贵又浪费资源。<strong>在连接池中，创建连接后，将其放置在池中，并再次使用它，因此不必建立新的连接。如果使用了所有连接，则会建立一个新连接并将其添加到池中。</strong> 连接池还减少了用户必须等待建立与数据库的连接的时间。</p>
  </li>
  <li>
    <p>分库分表之后,id 主键如何处理？</p>

    <p>因为要是分成多个表之后，每个表都是从 1 开始累加，这样是不对的，我们需要一个全局唯一的 id 来支持。</p>

    <p>生成全局 id 有下面这几种方式：</p>

    <ul>
      <li>UUID：不适合作为主键，因为太长了，并且无序不可读，查询效率低。比较适合用于生成唯一的名字的标示比如文件的名字。</li>
      <li>数据库自增 id : 两台数据库分别设置不同步长，生成不重复ID的策略来实现高可用。这种方式生成的 id 有序，但是需要独立部署数据库实例，成本高，还会有性能瓶颈。</li>
      <li>利用 redis 生成 id : 性能比较好，灵活方便，不依赖于数据库。但是，引入了新的组件造成系统更加复杂，可用性降低，编码更加复杂，增加了系统成本。</li>
      <li>Twitter的snowflake算法 ：Github 地址：https://github.com/twitter-archive/snowflake。</li>
      <li>美团的Leaf分布式ID生成系统 ：Leaf 是美团开源的分布式ID生成器，能保证全局唯一性、趋势递增、单调递增、信息安全，里面也提到了几种分布式方案的对比，但也需要依赖关系数据库、Zookeeper等中间件。感觉还不错。美团技术团队的一篇文章：https://tech.meituan.com/2017/04/21/mt-leaf.html 。</li>
      <li>……</li>
    </ul>

    <blockquote>
      <p><a href="https://snailclimb.gitee.io/javaguide/#/docs/system-design/micro-service/%E5%88%86%E5%B8%83%E5%BC%8Fid%E7%94%9F%E6%88%90%E6%96%B9%E6%A1%88%E6%80%BB%E7%BB%93">https://snailclimb.gitee.io/javaguide/#/docs/system-design/micro-service/%E5%88%86%E5%B8%83%E5%BC%8Fid%E7%94%9F%E6%88%90%E6%96%B9%E6%A1%88%E6%80%BB%E7%BB%93</a></p>
    </blockquote>

    <ul>
      <li>数据库自增</li>
      <li>数据库多主模式</li>
      <li>号端模式</li>
      <li>雪花模式</li>
      <li>Redis</li>
    </ul>
  </li>
  <li>
    <p>高性能优化规范建议</p>

    <p><strong>索引设计规范</strong></p>

    <ol>
      <li>
        <p>限制每张表上的索引数量,建议单张表索引不超过 5 个</p>

        <p>索引并不是越多越好！索引可以提高效率同样可以降低效率。</p>

        <p>索引可以增加查询效率，但同样也会降低插入和更新的效率，甚至有些情况下会降低查询效率。</p>

        <p>因为 MySQL 优化器在选择如何优化查询时，会根据统一信息，对每一个可以用到的索引来进行评估，以生成出一个最好的执行计划，如果同时有很多个索引都可以用于查询，就会增加 MySQL 优化器生成执行计划的时间，同样会降低查询性能。</p>
      </li>
      <li>
        <p>禁止给表中的每一列都建立单独的索引</p>

        <p>5.6 版本之前，一个 sql 只能使用到一个表中的一个索引，5.6 以后，虽然有了合并索引的优化方式，但是还是远远没有使用一个联合索引的查询方式好。</p>
      </li>
      <li>
        <p>每个 Innodb 表必须有个主键</p>

        <p>Innodb 是一种索引组织表：数据的存储的逻辑顺序和索引的顺序是相同的。每个表都可以有多个索引，但是表的存储顺序只能有一种。</p>

        <p>Innodb 是按照主键索引的顺序来组织表的</p>

        <p><strong>不要使用更新频繁的列作为主键</strong>，不适用多列主键（相当于联合索引）</p>

        <p><strong>不要使用 UUID,MD5,HASH,字符串列作为主键（无法保证数据的顺序增长）</strong></p>

        <p><strong>主键建议使用自增 ID 值</strong></p>
      </li>
      <li>
        <p>常见索引列建议</p>

        <p>出现在 SELECT、UPDATE、DELETE 语句的 <strong>WHERE 从句中的列</strong></p>

        <p><strong>包含在 ORDER BY、GROUP BY、DISTINCT 中的字段</strong></p>

        <p>并不要将符合 1 和 2 中的字段的列都建立一个索引， 通常将 1、2 中的字段建立联合索引效果更好</p>

        <p><strong>多表 join 的关联列</strong></p>
      </li>
      <li>
        <p>如何选择索引列的顺序</p>

        <p>建立索引的目的是：希望通过索引进行数据查找，减少随机 IO，增加查询性能 ，索引能过滤出越少的数据，则从磁盘中读入的数据也就越少。</p>

        <p><strong>区分度最高的放在联合索引的最左侧（区分度=列中不同值的数量/列的总行数）</strong></p>

        <p><strong>尽量把字段长度小的列放在联合索引的最左侧（因为字段长度越小，一页能存储的数据量越大，IO 性能也就越好）</strong></p>

        <p>使用最频繁的列放到联合索引的左侧（这样可以比较少的建立一些索引）</p>
      </li>
      <li>
        <p>避免建立冗余索引和重复索引（增加了查询优化器生成执行计划的时间）</p>

        <p>重复索引示例：primary key(id)、index(id)、unique index(id)</p>

        <p>冗余索引示例：index(a,b,c)、index(a,b)、index(a)</p>
      </li>
      <li>
        <p>对于频繁的查询优先考虑使用覆盖索引</p>

        <p>覆盖索引：就是包含了所有查询字段 (where,select,ordery by,group by 包含的字段) 的索引</p>

        <p>覆盖索引的好处：</p>

        <p>避免 Innodb 表进行索引的二次查询: Innodb 是以聚集索引的顺序来存储的，对于 Innodb 来说，二级索引在叶子节点中所保存的是行的主键信息，如果是用二级索引查询数据的话，在查找到相应的键值后，还要通过主键进行二次查询才能获取我们真实所需要的数据。而在覆盖索引中，二级索引的键值中可以获取所有的数据，避免了对主键的二次查询 ，减少了 IO 操作，提升了查询效率。</p>

        <p>可以把随机 IO 变成顺序 IO 加快查询效率: 由于覆盖索引是按键值的顺序存储的，对于 IO 密集型的范围查找来说，对比随机从磁盘读取每一行的数据 IO 要少的多，因此利用覆盖索引在访问时也可以把磁盘的随机读取的 IO 转变成索引查找的顺序 IO。</p>
      </li>
      <li>
        <p>索引 SET 规范</p>

        <p>尽量避免使用外键约束</p>

        <p>不建议使用外键约束（foreign key），但一定要在表与表之间的关联键上建立索引</p>

        <p>外键可用于保证数据的参照完整性，但建议在业务端实现</p>

        <p>外键会影响父表和子表的写操作从而降低性能</p>
      </li>
    </ol>
  </li>
  <li>
    <p>数据库索引总结</p>

    <p><strong>为什么要使用索引</strong></p>

    <ul>
      <li>通过创建唯一性索引，可以保证数据库每一行数据的唯一性。</li>
      <li>可以大大加快数据的检索速度（大大减少的检索的数据量），这也是创建索引的最主要的原因。</li>
    </ul>

    <p><strong>索引有这么多优点，为什么不对表中的每一个列创建一个索引呢</strong></p>

    <ul>
      <li>当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。</li>
      <li>索引需要占物理空间，除了数据表占数据空间之外，每一个索引还有占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。</li>
      <li>创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。</li>
    </ul>

    <p><strong>使用索引的注意事项</strong></p>

    <ul>
      <li><strong>在经常使用在WHERE子句中的列上面创建索引</strong>，加快条件的判断速度。</li>
      <li><strong>在经常需要排序的列上创建索引</strong>，因为索引已经排序，这样查询可以利用索引的排序，加快排序时间。</li>
      <li>对于中到大型表索引都是非常有效的，但是特大型表的话维护开销会很大，不适合建索引。</li>
      <li><strong>在经常用在连接的列上，这些列主要是一些外键</strong>，可以加快连接速度。</li>
      <li>避免where子句中对字段施加函数，这会造成无法命中索引。</li>
      <li><strong>将打算加索引的列设置为NOT NULL，否则将导致引擎放弃使用索引而进行全表扫描。</strong></li>
      <li>在使用limit offset查询缓慢时，可以借助索引来提高性能。</li>
    </ul>

    <p><strong>覆盖索引介绍</strong></p>

    <ul>
      <li>
        <p>什么是覆盖索引</p>

        <p>如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为“覆盖索引”。我们知道在InnoDB存储引擎中，如果不是主键索引，叶子结点存储的是主键+列值。最终还是要“回表”，也就是要通过主键再查找一次，这样就会比较慢。覆盖索引就是要查询出的列和索引是对应的，不做回表操作。</p>
      </li>
      <li>
        <p>覆盖索引使用实例</p>

        <p>现在我创建了索引(username, age)，在查询数据的时候：<code class="highlighter-rouge">select username, age from user where username = 'Java' and age = 22</code>，要查询出的列在叶子结点都存在，所以就不用回表。</p>
      </li>
    </ul>

    <p><strong>选择索引和编写利用这些索引的查询的3个原则</strong></p>

    <ol>
      <li>
        <p>单个访问时很慢的。如果服务器从存储中读取一个数据块知识为了获取其中一行，那么就浪费了很多工作。最好读取的块中能包含尽可能多所需要的行。使用索引可以创建位置引，用以提高效率。</p>
      </li>
      <li>
        <p>按顺序访问范围数据是很快的，这有两个原因。第一，顺序I/O不需要多次磁盘寻道，所以比随机I/O要快很多。第二，如果服务器能够按需要顺序读取数据，那么就不再需要额外的排序操作，并且GROUPBY查询也无需再做排序和将行按组进行聚合计算了。</p>
      </li>
      <li>
        <p>索引覆盖查询是很快的。如果一个索引包含了查询需要的所有列，那么查询引擎就不需要再回表查找行。这避免了大量的单行访问，而上面的第1点已经写明单行访问时很慢的。</p>
      </li>
    </ol>
  </li>
</ul>

<blockquote>
  <p><a href="https://www.nowcoder.com/discuss/356120">https://www.nowcoder.com/discuss/356120</a></p>
</blockquote>

<ul>
  <li>
    <p>Mysql(innondb 下同) 有哪几种事务隔离级别？</p>

    <p>读取未提交、读取已提交、可重复读、可串行化</p>
  </li>
  <li>
    <p>不同事务隔离级别分别会加哪些锁？</p>

    <p><strong>读取已提交级别：读取不加锁，写入、修改、删除加行锁。</strong> 读（快照读）存在不可重复读的问题，解决方法是读取数据后，将这些数据加行锁。写（当前读）存在幻读的问题，解决方法是使用next-key锁。</p>

    <p><strong>可重复读级别：next-key锁=gap锁+行锁。</strong> 行锁防止别的事务修改或删除，GAP锁防止别的事务新增，行锁和GAP锁结合形成的的Next-Key锁共同解决了RR级别在写数据时的幻读问题。</p>

    <p><strong>可串行化：读加共享锁，写加排他锁，读写互斥。并发能力非常差。</strong></p>

    <blockquote>
      <p><a href="https://tech.meituan.com/2014/08/20/innodb-lock.html">https://tech.meituan.com/2014/08/20/innodb-lock.html</a></p>
    </blockquote>

    <ul>
      <li>
        <p>读取已提交 Read Committed</p>

        <p>在RC级别中，数据的读取都是不加锁的，但是数据的写入、修改和删除是需要加锁的。</p>

        <table>
          <thead>
            <tr>
              <th>事务A</th>
              <th>事务B</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>begin;</td>
              <td>begin;</td>
            </tr>
            <tr>
              <td>update class_teacher set class_name=‘初三二班’ where teacher_id=1;</td>
              <td>update class_teacher set class_name=‘初三三班’ where teacher_id=1;</td>
            </tr>
            <tr>
              <td>commit;</td>
              <td> </td>
            </tr>
          </tbody>
        </table>

        <p>为了防止并发过程中的修改冲突，事务A中MySQL给teacher_id=1的数据行加锁，并一直不commit（释放锁），那么事务B也就一直拿不到该行锁，wait直到超时。</p>

        <p>这时我们要注意到，teacher_id是有索引的，如果是没有索引的class_name呢？update class_teacher set teacher_id=3 where class_name = ‘初三一班’; 那么MySQL会给整张表的所有数据行的加行锁。这里听起来有点不可思议，但是当sql运行的过程中，MySQL并不知道哪些数据行是 class_name = ‘初三一班’的（没有索引嘛），如果一个条件无法通过索引快速过滤，存储引擎层面就会将所有记录加锁后返回，再由MySQL Server层进行过滤。</p>

        <p>但在实际使用过程当中，MySQL做了一些改进，在MySQL Server过滤条件，发现不满足后，会调用unlock_row方法，把不满足条件的记录释放锁 (违背了二段锁协议的约束)。这样做，保证了最后只会持有满足条件记录上的锁，但是每条记录的加锁操作还是不能省略的。可见即使是MySQL，为了效率也是会违反规范的。（参见《高性能MySQL》中文第三版p181）</p>

        <p>这种情况同样适用于MySQL的默认隔离级别RR。所以对一个数据量很大的表做批量修改的时候，如果无法使用相应的索引，MySQL Server过滤数据的的时候特别慢，就会出现虽然没有修改某些行的数据，但是它们还是被锁住了的现象。</p>
      </li>
      <li>
        <p>可重复读 Repeatable Read</p>

        <p>这是MySQL中InnoDB默认的隔离级别。我们姑且分“读”和“写”两个模块来讲解。</p>

        <p><strong>读</strong></p>

        <p>RC（不可重读）模式下的展现</p>

        <table>
          <thead>
            <tr>
              <th>事务A</th>
              <th>事务B</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>begin;</td>
              <td>begin;</td>
            </tr>
            <tr>
              <td>select id,class_name,teacher_id from class_teacher where teacher_id=1; ①</td>
              <td> </td>
            </tr>
            <tr>
              <td> </td>
              <td>update class_teacher set class_name=’初三三班’ where id=1;</td>
            </tr>
            <tr>
              <td> </td>
              <td>commit;</td>
            </tr>
            <tr>
              <td>select id,class_name,teacher_id from class_teacher where teacher_id=1; ②</td>
              <td> </td>
            </tr>
            <tr>
              <td>commit;</td>
              <td> </td>
            </tr>
          </tbody>
        </table>

        <p>①
| id | class_name | teacher_id |
| – | – | – |
| 1 | 初三二班 | 1 |
| 2 | 初三一班 | 1 |</p>

        <p>②
| id | class_name | teacher_id |
| – | – | – |
| 1 | 初三三班 | 1 |
| 2 | 初三一班 | 1 |</p>

        <p>读到了事务B修改的数据，和第一次查询的结果不一样，是不可重读的。</p>

        <p>我们注意到，当teacher_id=1时，事务A先做了一次读取，事务B中间修改了id=1的数据，并commit之后，事务A第二次读到的数据和第一次完全相同。所以说它是可重读的。那么MySQL是怎么做到的呢？</p>

        <blockquote>
          <p>不可重复读和幻读的区别</p>

          <p>很多人容易搞混不可重复读和幻读，确实这两者有些相似。但不可重复读重点在于update和delete，而幻读的重点在于insert。</p>

          <p>如果使用锁机制来实现这两种隔离级别，在可重复读中，该sql第一次读取到数据后，就将这些数据加锁，其它事务无法修改这些数据，就可以实现可重复读了。但这种方法却无法锁住insert的数据，所以当事务A先前读取了数据，或者修改了全部数据，事务B还是可以insert数据提交，这时事务A就会发现莫名其妙多了一条之前没有的数据，这就是幻读，不能通过行锁来避免。需要Serializable隔离级别 ，读用读锁，写用写锁，读锁和写锁互斥，这么做可以有效的避免幻读、不可重复读、脏读等问题，但会极大的降低数据库的并发能力。</p>

          <p>所以说不可重复读和幻读最大的区别，就在于如何通过锁机制来解决他们产生的问题。</p>
        </blockquote>

        <p><strong>写</strong></p>

        <blockquote>
          <p>对于读取历史数据的方式，我们叫它快照读 (snapshot read)，而读取数据库当前版本数据的方式，叫当前读 (current read)。</p>

          <ul>
            <li>快照读：就是select
              <ul>
                <li>select * from table ….;</li>
              </ul>
            </li>
            <li>当前读：特殊的读操作，插入/更新/删除操作，属于当前读，处理的都是当前的数据，需要加锁。
              <ul>
                <li>select * from table where ? lock in share mode;</li>
                <li>select * from table where ? for update;</li>
                <li>insert;</li>
                <li>update;</li>
                <li>delete;</li>
              </ul>
            </li>
          </ul>
        </blockquote>

        <p>事务的隔离级别中虽然只定义了读数据的要求，实际上这也可以说是写数据的要求。上文的“读”，实际是讲的快照读；而这里说的“写”就是当前读了。</p>

        <p>为了解决当前读中的幻读问题，MySQL事务使用了Next-Key锁。</p>

        <p>Next-Key锁是行锁和GAP（间隙锁）的合并，行锁上文已经介绍了，接下来说下GAP间隙锁。</p>

        <p>行锁可以防止不同事务版本的数据修改提交时造成数据冲突的情况。但如何避免别的事务插入数据就成了问题。我们可以看看RR级别和RC级别的对比</p>

        <p>RC级别：</p>

        <table>
          <thead>
            <tr>
              <th>事务A</th>
              <th>事务B</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>begin;</td>
              <td>begin;</td>
            </tr>
            <tr>
              <td>select id,class_name,teacher_id from class_teacher where teacher_id=30; ①</td>
              <td> </td>
            </tr>
            <tr>
              <td>update class_teacher set class_name=’初三四班’ where teacher_id=30;</td>
              <td> </td>
            </tr>
            <tr>
              <td> </td>
              <td>insert into class_teacher values (null,’初三二班’,30);commit;</td>
            </tr>
            <tr>
              <td>select id,class_name,teacher_id from class_teacher where teacher_id=30; ②</td>
              <td> </td>
            </tr>
          </tbody>
        </table>

        <p>①</p>

        <table>
          <thead>
            <tr>
              <th>id</th>
              <th>class_name</th>
              <th>teacher_id</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>2</td>
              <td>初三二班</td>
              <td>30</td>
            </tr>
          </tbody>
        </table>

        <p>②</p>

        <table>
          <thead>
            <tr>
              <th>id</th>
              <th>class_name</th>
              <th>teacher_id</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>2</td>
              <td>初三二班</td>
              <td>30</td>
            </tr>
            <tr>
              <td>10</td>
              <td>初三二班</td>
              <td>30</td>
            </tr>
          </tbody>
        </table>

        <p>RR级别：</p>

        <table>
          <thead>
            <tr>
              <th>事务A</th>
              <th>事务B</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>begin;</td>
              <td>begin;</td>
            </tr>
            <tr>
              <td>select id,class_name,teacher_id from class_teacher where teacher_id=30; ①</td>
              <td> </td>
            </tr>
            <tr>
              <td>update class_teacher set class_name=’初三四班’ where teacher_id=30;</td>
              <td> </td>
            </tr>
            <tr>
              <td> </td>
              <td>insert into class_teacher values (null,’初三二班’,30);waiting….</td>
            </tr>
            <tr>
              <td>select id,class_name,teacher_id from class_teacher where teacher_id=30; ②</td>
              <td> </td>
            </tr>
            <tr>
              <td>commit;</td>
              <td>事务Acommit后，事务B的insert执行。</td>
            </tr>
          </tbody>
        </table>

        <p>①</p>

        <table>
          <thead>
            <tr>
              <th>id</th>
              <th>class_name</th>
              <th>teacher_id</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>2</td>
              <td>初三二班</td>
              <td>30</td>
            </tr>
          </tbody>
        </table>

        <p>②</p>

        <table>
          <thead>
            <tr>
              <th>id</th>
              <th>class_name</th>
              <th>teacher_id</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>2</td>
              <td>初三二班</td>
              <td>30</td>
            </tr>
          </tbody>
        </table>

        <p>通过对比我们可以发现，在RC级别中，事务A修改了所有teacher_id=30的数据，但是当事务Binsert进新数据后，事务A发现莫名其妙多了一行teacher_id=30的数据，而且没有被之前的update语句所修改，这就是“当前读”的幻读。</p>

        <p>RR级别中，事务A在update后加锁，事务B无法插入新数据，这样事务A在update前后读的数据保持一致，避免了幻读。这个锁，就是Gap锁。</p>

        <p>MySQL是这么实现的：</p>

        <p>在class_teacher这张表中，teacher_id是个索引，那么它就会维护一套B+树的数据关系，为了简化，我们用链表结构来表达（实际上是个树形结构，但原理相同）</p>

        <p><img src="https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/b3b6a55f.png" alt="img" /></p>

        <p>如图所示，InnoDB使用的是聚集索引，teacher_id身为二级索引，就要维护一个索引字段和主键id的树状结构（这里用链表形式表现），并保持顺序排列。</p>

        <p>Innodb将这段数据分成几个个区间</p>

        <ul>
          <li>(negative infinity, 5],</li>
          <li>(5,30],</li>
          <li>(30,positive infinity)；</li>
        </ul>

        <p>update class_teacher set class_name=‘初三四班’ where teacher_id=30;不仅用行锁，锁住了相应的数据行；同时也在两边的区间，（5,30]和（30，positive infinity），都加入了gap锁。这样事务B就无法在这个两个区间insert进新数据。</p>

        <p>受限于这种实现方式，Innodb很多时候会锁住不需要锁的区间。如下所示：</p>

        <table>
          <thead>
            <tr>
              <th>事务A</th>
              <th>事务B</th>
              <th>事务C</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>begin;</td>
              <td>begin;</td>
              <td>begin;</td>
            </tr>
            <tr>
              <td>select id,class_name,teacher_id from class_teacher; ①</td>
              <td> </td>
              <td> </td>
            </tr>
            <tr>
              <td>update class_teacher set class_name=’初一一班’ where teacher_id=20;</td>
              <td> </td>
              <td> </td>
            </tr>
            <tr>
              <td> </td>
              <td>insert into class_teacher values (null,’初三五班’,10);waiting …..</td>
              <td>insert into class_teacher values (null,’初三五班’,40);</td>
            </tr>
            <tr>
              <td>commit;</td>
              <td>事务A commit之后，这条语句才插入成功</td>
              <td>commit;</td>
            </tr>
            <tr>
              <td> </td>
              <td>commit;</td>
              <td> </td>
            </tr>
          </tbody>
        </table>

        <p>①</p>

        <table>
          <thead>
            <tr>
              <th>id</th>
              <th>class_name</th>
              <th>teacher_id</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>1</td>
              <td>初三一班</td>
              <td>5</td>
            </tr>
            <tr>
              <td>2</td>
              <td>初三二班</td>
              <td>30</td>
            </tr>
          </tbody>
        </table>

        <p>update的teacher_id=20是在(5，30]区间，即使没有修改任何数据，Innodb也会在这个区间加gap锁，而其它区间不会影响，事务C正常插入。</p>

        <p>如果使用的是没有索引的字段，比如update class_teacher set teacher_id=7 where class_name=‘初三八班（即使没有匹配到任何数据）’,那么会给全表加入gap锁。同时，它不能像上文中行锁一样经过MySQL Server过滤自动解除不满足条件的锁，因为没有索引，则这些字段也就没有排序，也就没有区间。除非该事务提交，否则其它事务无法插入任何数据。</p>

        <p>行锁防止别的事务修改或删除，GAP锁防止别的事务新增，行锁和GAP锁结合形成的的Next-Key锁共同解决了RR级别在写数据时的幻读问题。</p>
      </li>
      <li>
        <p>可串行化 Serializable</p>

        <p>这个级别很简单，读加共享锁，写加排他锁，读写互斥。使用的悲观锁的理论，实现简单，数据更加安全，但是并发能力非常差。如果你的业务并发的特别少或者没有并发，同时又要求数据及时可靠的话，可以使用这种模式。</p>
      </li>
    </ul>
  </li>
  <li>
    <p>MVCC</p>

    <blockquote>
      <p><a href="https://www.jianshu.com/p/8845ddca3b23">https://www.jianshu.com/p/8845ddca3b23</a></p>
    </blockquote>

    <blockquote>
      <p><a href="https://blog.csdn.net/nmjhehe/article/details/98470570">https://blog.csdn.net/nmjhehe/article/details/98470570</a></p>
    </blockquote>

    <blockquote>
      <p><a href="https://www.jianshu.com/p/57c510f4ec28">https://www.jianshu.com/p/57c510f4ec28</a></p>
    </blockquote>
  </li>
  <li>
    <p>快照读、当前读</p>

    <blockquote>
      <p><a href="https://blog.csdn.net/silyvin/article/details/79280934">https://blog.csdn.net/silyvin/article/details/79280934</a></p>
    </blockquote>

    <p>快照读：select …</p>

    <p>在rr级别下，mvcc完全解决了重复读，但并不能真正的完全避免幻读，只是在部分场景下利用历史数据规避了幻读</p>

    <p>当前读：select … lock in share mode (共享锁), select … for update (排他锁), update, insert, delete</p>

    <p>要完全避免，需要手动加锁将快照读调整使用next-key完全避免了幻读</p>
  </li>
  <li>
    <p>mysql的行锁、表锁、间隙锁、意向锁分别是做什么的？</p>

    <blockquote>
      <p><a href="https://blog.csdn.net/u010841296/article/details/84204701">https://blog.csdn.net/u010841296/article/details/84204701</a></p>
    </blockquote>

    <ul>
      <li>
        <p>共享锁Shared Locks（S锁）</p>

        <p>1、兼容性：加了S锁的记录，允许其他事务再加S锁，不允许其他事务再加X锁</p>

        <p>2、加锁方式：select…lock in share mode</p>
      </li>
      <li>
        <p>排他锁Exclusive Locks（X锁）</p>

        <p>1、兼容性：加了X锁的记录，不允许其他事务再加S锁或者X锁</p>

        <p>2、加锁方式：select…for update</p>
      </li>
      <li>
        <p>表锁：意向锁 Intention Locks，意向锁相互兼容</p>

        <p>1、表明“某个事务正在某些行持有了锁、或该事务准备去持有锁”</p>

        <p>2、意向锁的存在是为了协调行锁和表锁的关系，支持多粒度（表锁与行锁）的锁并存。</p>

        <p>3、<strong>例子：事务A修改user表的记录r，会给记录r上一把行级的排他锁（X），同时会给user表上一把意向排他锁（IX），这时事务B要给user表上一个表级的排他锁就会被阻塞。意向锁通过这种方式实现了行锁和表锁共存且满足事务隔离性的要求。</strong></p>

        <p>4、</p>

        <p>1）<strong>意向共享锁（IS锁）：事务在请求S锁前，要先获得IS锁</strong></p>

        <p>2）<strong>意向排他锁（IX锁）：事务在请求X锁前，要先获得IX锁</strong></p>

        <p>q1：<strong>为什么意向锁是表级锁呢？</strong></p>

        <p>当我们需要加一个排他锁时，需要根据意向锁去判断表中有没有数据行被锁定（行锁）：</p>

        <p>（1）如果意向锁是行锁，则需要遍历每一行数据去确认；</p>

        <p>（2）如果意向锁是表锁，则只需要判断一次即可知道有没数据行被锁定，提升性能。</p>

        <p>q2：<strong>意向锁怎么支持表锁和行锁并存？</strong></p>

        <p>（1）首先明确并存的概念是指数据库同时支持表、行锁，而不是任何情况都支持一个表中同时有一个事务A持有行锁、又有一个事务B持有表锁，因为表一旦被上了一个表级的写锁，肯定不能再上一个行级的锁。</p>

        <p>（2）<strong>如果事务A对某一行上锁，其他事务就不可能修改这一行。这与“事务B锁住整个表就能修改表中的任意一行”形成了冲突。所以，没有意向锁的时候，让行锁与表锁共存，就会带来很多问题。于是有了意向锁的出现，如q1的答案中，数据库不需要在检查每一行数据是否有锁，而是直接判断一次意向锁是否存在即可，能提升很多性能。</strong></p>
      </li>
      <li>
        <p>行锁：记录锁(Record Locks)</p>

        <p>（1）记录锁, 仅仅锁住索引记录的一行，在单条索引记录上加锁。</p>

        <p>（2）record lock锁住的永远是索引，而非记录本身，即使该表上没有任何索引，那么innodb会在后台创建一个隐藏的聚集主键索引，那么锁住的就是这个隐藏的聚集主键索引。
所以说当一条sql没有走任何索引时，那么将会在每一条聚合索引后面加X锁，这个类似于表锁，但原理上和表锁应该是完全不同的。</p>
      </li>
      <li>
        <p>行锁：间隙锁(Gap Locks)</p>

        <p>（1）区间锁, 仅仅锁住一个索引区间（开区间，不包括双端端点）。</p>

        <p>（2）在索引记录之间的间隙中加锁，或者是在某一条索引记录之前或者之后加锁，并不包括该索引记录本身。比如在 1、2、3中，间隙锁的可能值有 (∞, 1)，(1, 2)，(2, ∞)。</p>

        <p>（3）间隙锁可用于防止幻读，保证索引间的不会被插入数据</p>
      </li>
      <li>
        <p>行锁：临键锁(Next-Key Locks)</p>

        <p>（1）record lock + gap lock, 左开右闭区间。</p>

        <p>（2）默认情况下，innodb使用next-key locks来锁定记录。select … for update</p>

        <p>（3）但当查询的索引含有唯一属性的时候，Next-Key Lock 会进行优化，将其降级为Record Lock，即仅锁住索引本身，不是范围。</p>

        <p>（4）Next-Key Lock在不同的场景中会退化:</p>

        <p><img src="https://img-blog.csdnimg.cn/20181118210006461.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA4NDEyOTY=,size_16,color_FFFFFF,t_70" alt="img" /></p>
      </li>
    </ul>
  </li>
  <li>
    <p>说说什么是最左匹配？</p>

    <blockquote>
      <p><a href="https://segmentfault.com/a/1190000015416513">https://segmentfault.com/a/1190000015416513</a></p>
    </blockquote>

    <p><strong>最左前缀匹配原则</strong></p>

    <p>在mysql建立联合索引时会遵循最左前缀匹配的原则，即最左优先，在检索数据时从联合索引的最左边开始匹配，示例：
对列col1、列col2和列col3建一个联合索引</p>

    <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="k">KEY</span> <span class="n">test_col1_col2_col3</span> <span class="k">on</span> <span class="n">test</span><span class="p">(</span><span class="n">col1</span><span class="p">,</span><span class="n">col2</span><span class="p">,</span><span class="n">col3</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>

    <p>联合索引 test_col1_col2_col3 实际建立了(col1)、(col1,col2)、(col,col2,col3)三个索引。</p>

    <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">test</span> <span class="k">WHERE</span> <span class="n">col1</span><span class="o">=</span><span class="err">“</span><span class="mi">1</span><span class="err">”</span> <span class="k">AND</span> <span class="n">clo2</span><span class="o">=</span><span class="err">“</span><span class="mi">2</span><span class="err">”</span> <span class="k">AND</span> <span class="n">clo4</span><span class="o">=</span><span class="err">“</span><span class="mi">4</span><span class="err">”</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>

    <p>上面这个查询语句执行时会依照最左前缀匹配原则，检索时会使用索引(col1,col2)进行数据匹配。</p>

    <p><strong>注意</strong></p>

    <p>索引的字段可以是任意顺序的，如：</p>

    <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">test</span> <span class="k">WHERE</span> <span class="n">col1</span><span class="o">=</span><span class="err">“</span><span class="mi">1</span><span class="err">”</span> <span class="k">AND</span> <span class="n">clo2</span><span class="o">=</span><span class="err">“</span><span class="mi">2</span><span class="err">”</span>
<span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">test</span> <span class="k">WHERE</span> <span class="n">col2</span><span class="o">=</span><span class="err">“</span><span class="mi">2</span><span class="err">”</span> <span class="k">AND</span> <span class="n">clo1</span><span class="o">=</span><span class="err">“</span><span class="mi">1</span><span class="err">”</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>

    <p>这两个查询语句都会用到索引(col1,col2)，mysql创建联合索引的规则是首先会对联合合索引的最左边的，也就是第一个字段col1的数据进行排序，在第一个字段的排序基础上，然后再对后面第二个字段col2进行排序。其实就相当于实现了类似 order by col1 col2这样一种排序规则。</p>

    <p>有人会疑惑第二个查询语句不符合最左前缀匹配：首先可以肯定是两个查询语句都包含索引(col1,col2)中的col1、col2两个字段，只是顺序不一样，查询条件一样，最后所查询的结果肯定是一样的。既然结果是一样的，到底以何种顺序的查询方式最好呢？此时我们可以借助mysql查询优化器explain，explain会纠正sql语句该以什么样的顺序执行效率最高，最后才生成真正的执行计划。</p>

    <p><strong>为什么要使用联合索引</strong></p>

    <ul>
      <li>减少开销。建一个联合索引(col1,col2,col3)，实际相当于建了(col1),(col1,col2),(col1,col2,col3)三个索引。每多一个索引，都会增加写操作的开销和磁盘空间的开销。对于大量数据的表，使用联合索引会大大的减少开销！</li>
      <li>覆盖索引。对联合索引(col1,col2,col3)，如果有如下的sql: select col1,col2,col3 from test where col1=1 and col2=2。那么MySQL可以直接通过遍历索引取得数据，而无需回表，这减少了很多的随机io操作。减少io操作，特别的随机io其实是dba主要的优化策略。所以，在真正的实际应用中，覆盖索引是主要的提升性能的优化手段之一。</li>
      <li>效率高。索引列越多，通过索引筛选出的数据越少。有1000W条数据的表，有如下sql:select from table where col1=1 and col2=2 and col3=3,假设假设每个条件可以筛选出10%的数据，如果只有单值索引，那么通过该索引能筛选出1000W10%=100w条数据，然后再回表从100w条数据中找到符合col2=2 and col3= 3的数据，然后再排序，再分页；如果是联合索引，通过索引筛选出1000w10% 10% *10%=1w，效率提升可想而知！</li>
    </ul>

    <blockquote>
      <p><a href="https://segmentfault.com/a/1190000015416513">https://segmentfault.com/a/1190000015416513</a></p>
    </blockquote>
  </li>
  <li>
    <p>如何定位慢SQL</p>

    <blockquote>
      <p><a href="https://blog.csdn.net/zzti_erlie/article/details/80534733">https://blog.csdn.net/zzti_erlie/article/details/80534733</a></p>
    </blockquote>
  </li>
  <li>
    <p>索引失效</p>

    <blockquote>
      <p><a href="https://www.cnblogs.com/liehen2046/p/11052666.html">https://www.cnblogs.com/liehen2046/p/11052666.html</a></p>
    </blockquote>

    <ul>
      <li>
        <p>什么时候没用</p>

        <p>1.有or必全有索引;
2.<strong>复合索引未用左列字段;</strong>
3.<strong>like以%开头;</strong>
4.<strong>需要类型转换;</strong>
5.where中索引列有运算;
6.<strong>where中索引列使用了函数;</strong>
7.如果mysql觉得全表扫描更快时（数据少）;</p>
      </li>
      <li>
        <p>什么时候没必要用</p>

        <p>1.唯一性差;
2.频繁更新的字段不用（更新索引消耗）;
3.where中不用的字段;
4.索引使用&lt;&gt;时，效果一般;</p>
      </li>
    </ul>
  </li>
  <li>
    <p>explain</p>

    <blockquote>
      <p><a href="https://blog.csdn.net/zzti_erlie/article/details/80534733">https://blog.csdn.net/zzti_erlie/article/details/80534733</a></p>
    </blockquote>

    <ul>
      <li>id</li>
      <li>select_type: simple, [primary, subquery], [primary, derived (在from子句里)], [primary, union, union result]</li>
      <li>type: eq_ref, ref, range, index, all</li>
      <li>extra: using index, using where, using temporary</li>
    </ul>

    <blockquote>
      <p>高性能MySQL P200</p>
    </blockquote>

    <p>一般MysQL能够使用如下三种方式应用WHERE条件，从好到坏依次为：</p>

    <ul>
      <li>在索引中使用WHERE条件来过滤不匹配的记录。</li>
      <li>使用索引覆盖扫描（在Extra列中出现了Using index）来返回记录，直接从索引中过滤不需要的记录并返回命重的结果。</li>
      <li>从数据表中返回数据，然后过滤不满足条件的记录（在Extra列中出现Using Where）。</li>
    </ul>
  </li>
  <li>
    <p>MySQL 执行流程</p>

    <blockquote>
      <p><a href="https://www.cnblogs.com/liyasong/p/mysql_zhixingguocheng.html">https://www.cnblogs.com/liyasong/p/mysql_zhixingguocheng.html</a></p>
    </blockquote>
  </li>
</ul>

<p>　　mysql得到sql语句后，大概流程如下：</p>

<p>　　1.sql的解析器：负责解析和转发sql</p>

<p>　　2.预处理器：对解析后的sql树进行验证</p>

<p>　　3.查询优化器：得到一个执行计划</p>

<p>　　4.查询执行引擎：得到数据结果集</p>

<p>　　5.将数据放回给调用端。</p>

<ul>
  <li>
    <p>如何优化慢查询？</p>

    <blockquote>
      <p><a href="https://database.51cto.com/art/201809/583239.htm">https://database.51cto.com/art/201809/583239.htm</a></p>
    </blockquote>

    <ul>
      <li>
        <p><strong>索引优化</strong></p>

        <p>索引类型</p>

        <ul>
          <li><strong>普通索引</strong>：是最基本的索引，它没有任何限制。</li>
          <li><strong>唯一索引</strong>：与前面的普通索引类似，不同的就是：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。</li>
          <li><strong>组合索引</strong>：指多个字段上创建的索引，只有在查询条件中使用了创建索引时的***个字段，索引才会被使用。</li>
          <li><strong>主键索引</strong>：是一种特殊的唯一索引，一个表只能有一个主键，不允许有空值。一般是在建表的时候同时创建主键索引</li>
          <li><strong>全文索引</strong>：主要用来查找文本中的关键字，而不是直接与索引中的值相比较。fulltext索引跟其它索引大不相同，它更像是一个搜索引擎，而不是简单的where语句的参数匹配。fulltext索引配合match against操作使用，而不是一般的where语句加like。它可以在create table，alter table ，create index使用，不过目前只有char、varchar，text 列上可以创建全文索引。值得一提的是，在数据量较大时候，现将数据放入一个没有全局索引的表中，然后再用CREATE index创建fulltext索引，要比先为一张表建立fulltext然后再将数据写入的速度快很多。</li>
        </ul>

        <p>优化原则</p>

        <ul>
          <li><strong>只要列中含有NULL值，就不要在此例设置索引</strong>，复合索引如果有NULL值，此列在使用时也不会使用索引</li>
          <li>尽量使用短索引，如果可以，应该制定一个前缀长度</li>
          <li><strong>对于经常在where子句使用的列设置索引</strong>，这样会加快查找速度</li>
          <li><strong>对于有多个列where或者order by子句的，应该建立复合索引</strong></li>
          <li><strong>对于like语句，以%或者‘-’开头的不会使用索引，以%结尾会使用索引</strong></li>
          <li><strong>尽量不要在列上进行运算（函数操作和表达式操作）</strong></li>
          <li>尽量不要使用not in和&lt;&gt;操作</li>
        </ul>
      </li>
      <li>
        <p><strong>SQL语句优化</strong></p>

        <p>优化原则</p>

        <ul>
          <li>查询时，<strong>能不要*就不用*，尽量写全字段名</strong></li>
          <li><strong>大部分情况连接效率远大于子查询</strong></li>
          <li>多使用explain和profile分析查询语句</li>
          <li><strong>查看慢查询日志，找出执行时间长的sql语句优化</strong></li>
          <li>多表连接时，尽量<strong>小表驱动大表，即小表 join 大表</strong></li>
          <li><strong>在分页时使用limit</strong></li>
          <li>对于经常使用的查询，可以开启缓存</li>
        </ul>
      </li>
      <li>
        <p><strong>大表优化</strong></p>

        <p>数据表拆分：主要就是垂直拆分和水平拆分。</p>

        <ul>
          <li><strong>水平切分</strong>:将记录散列到不同的表中，各表的结构完全相同，每次从分表中查询, 提高效率。</li>
          <li><strong>垂直切分</strong>:将表中大字段单独拆分到另外一张表, 形成一对一的关系。</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>mysql索引为什么用的是b+ tree而不是b tree、红黑树</p>

    <blockquote>
      <p><a href="https://blog.csdn.net/qq_35923749/article/details/88068659">https://blog.csdn.net/qq_35923749/article/details/88068659</a></p>
    </blockquote>

    <p>B-树、B+树、红黑树，都是平衡查找树，那么查询效率上讲，平均都是O(logn)。使用什么哪种数据结构，肯定是出于提高数据库的查询效率的考虑。</p>

    <p><strong>一、B+树做索引而不用B-树</strong></p>

    <p>那么Mysql如何衡量查询效率呢？– 磁盘IO次数。</p>

    <p>一般来说索引非常大，尤其是关系性数据库这种数据量大的索引能达到亿级别，所以为了减少内存的占用，索引也会被存储在磁盘上。B-树/B+树 的特点就是每层节点数目非常多，层数很少，目的就是为了减少磁盘IO次数，但是B-树的每个节点都有data域（指针），这无疑增大了节点大小，说白了增加了磁盘IO次数（磁盘IO一次读出的数据量大小是固定的，单个数据变大，每次读出的就少，IO次数增多，一次IO多耗时），而B+树除了叶子节点其它节点并不存储数据，节点小，磁盘IO次数就少。</p>

    <ul>
      <li>
        <p>优点一： B+树只有叶节点存放数据，其余节点用来索引，而B-树是每个索引节点都会有Data域。</p>
      </li>
      <li>
        <p>优点二： B+树所有的Data域在叶子节点，并且所有叶子节点之间都有一个链指针。 这样遍历叶子节点就能获得全部数据，这样就能进行区间访问啦。在数据库中基于范围的查询是非常频繁的，而B树不支持这样的遍历操作。</p>
      </li>
    </ul>

    <p><strong>二、B+树做索引而不用红黑树</strong></p>

    <p>AVL 树（平衡二叉树）和红黑树（二叉查找树）基本都是存储在内存中才会使用的数据结构。在大规模数据存储的时候，红黑树往往出现由于树的深度过大而造成磁盘IO读写过于频繁，进而导致效率低下的情况。为什么会出现这样的情况，我们知道要获取磁盘上数据，必须先通过磁盘移动臂移动到数据所在的柱面，然后找到指定盘面，接着旋转盘面找到数据所在的磁道，最后对数据进行读写。磁盘IO代价主要花费在查找所需的柱面上，树的深度过大会造成磁盘IO频繁读写。根据磁盘查找存取的次数往往由树的高度所决定，所以，只要我们通过某种较好的树结构减少树的结构尽量减少树的高度，B树可以有多个子女，从几十到上千，可以降低树的高度。</p>

    <p>数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧：每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。</p>
  </li>
  <li>
    <p>分库分表如何选择分表键</p>

    <blockquote>
      <p><a href="https://github.com/Meituan-Dianping/Zebra/wiki/%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E5%88%86%E8%A1%A8%E9%94%AE%EF%BC%8C%E8%B7%AF%E7%94%B1%E8%A7%84%E5%88%99%E5%8F%8A%E5%88%86%E7%89%87%E6%95%B0">https://github.com/Meituan-Dianping/Zebra/wiki/%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E5%88%86%E8%A1%A8%E9%94%AE%EF%BC%8C%E8%B7%AF%E7%94%B1%E8%A7%84%E5%88%99%E5%8F%8A%E5%88%86%E7%89%87%E6%95%B0</a></p>
    </blockquote>

    <p><strong>分表键即分库/分表字段，是在水平拆分过程中用于生成拆分规则的数据表字段。</strong></p>

    <p>数据表拆分的首要原则，就是要尽可能找到数据表中的数据在业务逻辑上的主体，并确定大部分（或核心的）数据库操作都是围绕这个主体的数据进行，然后可使用该主体对应的字段作为分表键，进行分库分表。</p>

    <p>业务逻辑上的主体，通常与业务的应用场景相关，下面的一些典型应用场景都有明确的业务逻辑主体，可用于分表键：</p>

    <ul>
      <li>面向用户的互联网应用，都是围绕用户维度来做各种操作，那么业务逻辑主体就是用户，可使用用户对应的字段作为分表键；</li>
      <li>侧重于卖家的电商应用，都是围绕卖家维度来进行各种操作，那么业务逻辑主体就是卖家，可使用卖家对应的字段作为分表键；</li>
    </ul>

    <p>以此类推，其它类型的应用场景，大多也能找到合适的业务逻辑主体作为分表键的选择。</p>

    <p>如果确实找不到合适的业务逻辑主体作为分表键，那么可以考虑下面的方法来选择分表键：</p>

    <ul>
      <li>根据数据分布和访问的均衡度来考虑分表键，尽量将数据表中的数据相对均匀地分布在不同的物理分库/分表中，适用于大量分析型查询的应用场景（查询并发度大部分能维持为1）；</li>
      <li>按照数字（字符串）类型与时间类型字段相结合作为分表键，进行分库和分表，适用于日志检索类的应用场景。</li>
    </ul>

    <p>注意：无论选择什么拆分键，采用何种拆分策略，都要注意拆分值是否存在热点的问题，尽量规避热点数据来选择拆分键。</p>

    <p>注意：不一定需要拿数据库主键当做分表键，也可以拿其他业务值当分表键。拿主键当分表键的好处是可以散列均衡，减少热点问题。</p>
  </li>
  <li>
    <p>分库分表的情况下，查询时一般是如何做排序的？</p>

    <blockquote>
      <p><a href="https://cloud.tencent.com/developer/article/1404798">https://cloud.tencent.com/developer/article/1404798</a></p>
    </blockquote>

    <p><strong>Mysql分库分表方案</strong></p>

    <ul>
      <li>
        <p><strong>为什么要分表</strong></p>

        <p>当一张表的数据达到几千万时，你查询一次所花的时间会变多，如果有联合查询的话，我想有可能会死在那儿了。分表的目的就在于此，减小数据库的负担，缩短查询时间。</p>
      </li>
      <li>
        <p><strong>大数据量并且访问频繁的表，将其分为若干个表</strong></p>

        <p>比如对于某网站平台的数据库表-公司表，数据量很大，这种能预估出来的大数据量表，我们就事先分出个N个表，这个N是多少，根据实际情况而定。</p>

        <p>某网站现在的数据量至多是5000万条，可以设计每张表容纳的数据量是500万条，也就是拆分成10张表。</p>

        <p>那么如何判断某张表的数据是否容量已满呢?可以在程序段对于要新增数据的表，在插入前先做统计表记录数量的操作，当&lt;500万条数据，就直接插入，当已经到达阀值，可以在程序段新创建数据库表(或者已经事先创建好)，再执行插入操作。</p>
      </li>
      <li>
        <p><strong>利用merge存储引擎来实现分表</strong></p>

        <p>如果要把已有的大数据量表分开比较痛苦，最痛苦的事就是改代码，因为程序里面的sql语句已经写好了。用merge存储引擎来实现分表, 这种方法比较适合。</p>
      </li>
    </ul>

    <p><strong>数据库架构</strong></p>

    <ol>
      <li>
        <p>简单的MySQL主从复制</p>

        <p>MySQL的主从复制解决了数据库的读写分离，并很好的提升了读的性能，其图如下：</p>

        <p><img src="https://ask.qcloudimg.com/http-save/yehe-4878700/fqy4a9b1um.jpeg?imageView2/2/w/1620" alt="img" /></p>

        <p>其主从复制的过程如下图所示：</p>

        <p><img src="https://ask.qcloudimg.com/http-save/yehe-4878700/odg78hats0.jpeg?imageView2/2/w/1620" alt="img" /></p>

        <p>但是，主从复制也带来其他一系列性能瓶颈问题：</p>

        <ul>
          <li>写入无法扩展</li>
          <li>写入无法缓存</li>
          <li>复制延时</li>
          <li>锁表率上升</li>
          <li>表变大，缓存率下降</li>
        </ul>
      </li>
      <li>
        <p>MySQL垂直分区</p>

        <p>如果把业务切割得足够独立，那把不同业务的数据放到不同的数据库服务器将是一个不错的方案，而且万一其中一个业务崩溃了也不会影响其他业务的正常进行，并且也起到了负载分流的作用，大大提升了数据库的吞吐能力。经过垂直分区后的数据库架构图如下：</p>

        <p><img src="https://ask.qcloudimg.com/http-save/yehe-4878700/eiuihqsrb2.jpeg?imageView2/2/w/1620" alt="img" /></p>

        <p>然而，尽管业务之间已经足够独立了，但是有些业务之间或多或少总会有点联系，如用户，基本上都会和每个业务相关联，况且这种分区方式，也不能解决单张表数据量暴涨的问题，因此为何不试试水平分割呢?</p>
      </li>
      <li>
        <p>MySQL水平分片(Sharding)</p>

        <p>这是一个非常好的思路，<strong>将用户按一定规则（按id哈希）分组，并把该组用户的数据存储到一个数据库分片中，即一个sharding</strong>，这样随着用户数量的增加，只要简单地配置一台服务器即可，原理图如下：</p>

        <p><img src="https://ask.qcloudimg.com/http-save/yehe-4878700/gztxxds73x.jpeg?imageView2/2/w/1620" alt="img" /></p>

        <p>如何来确定某个用户所在的shard呢，可以建一张用户和shard对应的数据表，每次请求先从这张表找用户的shard id，再从对应shard中查询相关数据，如下图所示：</p>

        <p><img src="https://ask.qcloudimg.com/http-save/yehe-4878700/eyqabomfoa.jpeg?imageView2/2/w/1620" alt="img" /></p>
      </li>
    </ol>

    <p><strong>单库单表</strong></p>

    <p>单库单表是最常见的数据库设计，例如，有一张用户(user)表放在数据库db中，所有的用户都可以在db库中的user表中查到。</p>

    <p><strong>单库多表</strong></p>

    <p>随着用户数量的增加，user表的数据量会越来越大，当数据量达到一定程度的时候对user表的查询会渐渐的变慢，从而影响整个DB的性能。如果使用mysql, 还有一个更严重的问题是，当需要添加一列的时候，mysql会锁表，期间所有的读写操作只能等待。</p>

    <p>可以通过某种方式将user进行水平的切分，产生两个表结构完全一样的user_0000,user_0001等表，user_0000 + user_0001 + …的数据刚好是一份完整的数据。</p>

    <p><strong>多库多表</strong></p>

    <p>随着数据量增加也许单台DB的存储空间不够，随着查询量的增加单台数据库服务器已经没办法支撑。这个时候可以再对数据库进行水平区分。</p>

    <p><strong>分库分表规则</strong></p>

    <p>设计表的时候需要确定此表按照什么样的规则进行分库分表。例如，当有新用户时，程序得确定将此用户信息添加到哪个表中；同理，当登录的时候我们得通过用户的账号找到数据库中对应的记录，所有的这些都需要按照某一规则进行。</p>

    <p><strong>路由</strong></p>

    <p>通过分库分表规则查找到对应的表和库的过程。如分库分表的规则是user_id mod 4的方式，当用户新注册了一个账号，账号id的123,我们可以通过id mod 4的方式确定此账号应该保存到User_0003表中。当用户123登录的时候，我们通过123 mod 4后确定记录在User_0003中。</p>

    <p><strong>分库分表产生的问题，及注意事项</strong></p>

    <ol>
      <li>
        <p>分库分表维度的问题</p>

        <p>假如用户购买了商品,需要将交易记录保存取来，如果按照用户的维度分表，则每个用户的交易记录都保存在同一表中，所以很快很方便的查找到某用户的 购买情况，但是某商品被购买的情况则很有可能分布在多张表中，查找起来比较麻烦。反之，按照商品维度分表，可以很方便的查找到此商品的购买情况，但要查找 到买人的交易记录比较麻烦。</p>

        <p>所以常见的解决方式有：</p>

        <ul>
          <li>通过扫表的方式解决，此方法基本不可能，效率太低了。</li>
          <li>记录两份数据，一份按照用户纬度分表，一份按照商品维度分表。</li>
        </ul>

        <p>通过搜索引擎解决，但如果实时性要求很高，又得关系到实时搜索。</p>
      </li>
      <li>
        <p>联合查询的问题</p>

        <p>联合查询基本不可能，因为关联的表有可能不在同一数据库中。</p>
      </li>
      <li>
        <p>避免跨库事务</p>

        <p>避免在一个事务中修改db0中的表的时候同时修改db1中的表，一个是操作起来更复杂，效率也会有一定影响。</p>
      </li>
      <li>
        <p>尽量把同一组数据放到同一DB服务器上</p>

        <p>例如将卖家a的商品和交易信息都放到db0中，当db1挂了的时候，卖家a相关的东西可以正常使用。也就是说避免数据库中的数据依赖另一数据库中的数据。</p>
      </li>
    </ol>
  </li>
  <li>
    <p>数据库主从一致性</p>

    <blockquote>
      <p><a href="https://blog.csdn.net/qq_33363618/article/details/81236222">https://blog.csdn.net/qq_33363618/article/details/81236222</a></p>
    </blockquote>

    <p><strong>半同步复制</strong></p>

    <p>（1）系统先对DB-master进行了一个写操作，写主库</p>

    <p>（2）等主从同步完成，写主库的请求才返回</p>

    <p>（3）读从库，读到最新的数据（如果读请求先完成，写请求后完成，读取到的是“当时”最新的数据）</p>

    <p><strong>数据库中间件</strong></p>

    <p>（1）所有的读写都走数据库中间件，通常情况下，写请求路由到主库，读请求路由到从库</p>

    <p>（2）记录所有路由到写库的key，在经验主从同步时间窗口内（假设是500ms），如果有读请求访问中间件，此时有可能从库还是旧数据，就把这个key上的读请求路由到主库</p>

    <p>（3）经验主从同步时间过完后，对应key的读请求继续路由到从库</p>

    <p><strong>缓存记录写key法</strong></p>

    <p>当写请求发生的时候：</p>

    <p>（1）将某个库上的某个key要发生写操作，记录在cache里，并设置“经验主从同步时间”的cache超时时间，例如500ms</p>

    <p>（2）修改数据库</p>

    <p>而读请求发生的时候：</p>

    <p>（1）先到cache里查看，对应库的对应key有没有相关数据</p>

    <p>（2）如果cache hit，有相关数据，说明这个key上刚发生过写操作，此时需要将请求路由到主库读最新的数据</p>

    <p>（3）如果cache miss，说明这个key上近期没有发生过写操作，此时将请求路由到从库，继续读写分离</p>
  </li>
  <li>
    <p>mysql 索引 大于等于 走不走索引 最左前缀</p>

    <blockquote>
      <p><a href="https://blog.csdn.net/weixin_36429334/article/details/53945022">https://blog.csdn.net/weixin_36429334/article/details/53945022</a></p>
    </blockquote>
  </li>
  <li>
    <p>如何设计数据库表</p>

    <blockquote>
      <p><a href="https://blog.csdn.net/jiyiqinlovexx/article/details/44544325">https://blog.csdn.net/jiyiqinlovexx/article/details/44544325</a></p>
    </blockquote>

    <blockquote>
      <p><a href="https://www.jianshu.com/p/b3969c49dfaa">https://www.jianshu.com/p/b3969c49dfaa</a></p>
    </blockquote>

    <p>E-R图：实体、关系</p>

    <p>一对一（外键）、一对多（多的一方加外键）、多对多（中间表，两列作为联合主键）</p>

    <blockquote>
      <p><a href="https://www.cnblogs.com/wsg25/p/9615100.html">https://www.cnblogs.com/wsg25/p/9615100.html</a></p>
    </blockquote>

    <p>三大范式</p>

    <ul>
      <li>
        <p><strong>第一范式（1NF）：要求数据库表的每一列都是不可分割的原子数据项。</strong></p>
      </li>
      <li>
        <p>第二范式（2NF）：在1NF的基础上，非码属性必须完全依赖于候选码（在1NF基础上消除非主属性对主码的部分函数依赖）</p>

        <p><strong>第二范式需要确保数据库表中的每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）。</strong></p>
      </li>
      <li>
        <p>第三范式（3NF）：在2NF基础上，任何非主属性不依赖于其它非主属性（在2NF基础上消除传递依赖）</p>

        <p><strong>第三范式需要确保数据表中的每一列数据都和主键直接相关，而不能间接相关。</strong></p>
      </li>
    </ul>
  </li>
  <li>
    <p>MySQL死锁</p>

    <blockquote>
      <p><a href="https://www.cnblogs.com/zejin2008/p/5262751.html">https://www.cnblogs.com/zejin2008/p/5262751.html</a></p>
    </blockquote>
  </li>
  <li>
    <p>MySQL如何保证ACID</p>

    <blockquote>
      <p><a href="https://www.jianshu.com/p/7e5853d7087a">https://www.jianshu.com/p/7e5853d7087a</a></p>
    </blockquote>

    <ul>
      <li>原子性：undo log</li>
      <li>持久性：redo log</li>
      <li>隔离性：利用锁和MVCC机制</li>
    </ul>
  </li>
</ul>

<h4 id="redis">Redis</h4>

<ul>
  <li>
    <p>redis性能为什么高?</p>

    <blockquote>
      <p><a href="https://blog.csdn.net/xlgen157387/article/details/79470556">https://blog.csdn.net/xlgen157387/article/details/79470556</a></p>
    </blockquote>

    <p><strong>Redis到底有多快</strong></p>

    <p>可以达到100000+的QPS（每秒内查询次数）。</p>

    <p><img src="https://img-blog.csdn.net/2018030715491722?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg3MDUxOA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" /></p>

    <p>横轴是连接数，纵轴是QPS。</p>

    <p><strong>Redis为什么这么快</strong></p>

    <p>1、<strong>完全基于内存</strong>，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)；</p>

    <p>2、<strong>数据结构简单</strong>，对数据操作也简单，Redis中的数据结构是专门进行设计的；</p>

    <p>3、<strong>采用单线程</strong>，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；</p>

    <p>4、使用<strong>多路I/O复用模型</strong>，非阻塞IO；</p>

    <p>5、使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求；</p>

    <p>以上几点都比较好理解，下边我们针对多路 I/O 复用模型进行简单的探讨：</p>

    <p>多路 I/O 复用模型</p>

    <p>多路I/O复用模型是利用 select、poll、epoll 可以同时监察多个流的 I/O 事件的能力，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有 I/O 事件时，就从阻塞态中唤醒，于是程序就会轮询一遍所有的流（epoll 是只轮询那些真正发出了事件的流），并且只依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。</p>

    <blockquote>
      <p><a href="https://www.zhihu.com/question/28594409">https://www.zhihu.com/question/28594409</a></p>
    </blockquote>

    <blockquote>
      <p>下面举一个例子，模拟一个tcp服务器处理30个客户socket。
假设你是一个老师，让30个学生解答一道题目，然后检查学生做的是否正确，你有下面几个选择：</p>

      <ol>
        <li>
          <p>第一种选择：<strong>按顺序逐个检查</strong>，先检查A，然后是B，之后是C、D。。。这中间如果有一个学生卡主，全班都会被耽误。</p>

          <p>这种模式就好比，你用循环挨个处理socket，根本不具有并发能力。</p>
        </li>
        <li>第二种选择：你<strong>创建30个分身</strong>，每个分身检查一个学生的答案是否正确。 这种类似于为每一个用户创建一个进程或者线程处理连接。</li>
        <li>
          <p>第三种选择，你<strong>站在讲台上等，谁解答完谁举手</strong>。这时C、D举手，表示他们解答问题完毕，你下去依次检查C、D的答案，然后继续回到讲台上等。此时E、A又举手，然后去处理E和A。。。</p>

          <p>这种就是IO复用模型，Linux下的select、poll和epoll就是干这个的。将用户socket对应的fd注册进epoll，然后epoll帮你监听哪些socket上有消息到达，这样就避免了大量的无用操作。此时的socket应该采用<strong>非阻塞模式</strong>。</p>

          <p>这样，整个过程只在调用select、poll、epoll这些调用的时候才会阻塞，收发客户消息是不会阻塞的，整个进程或者线程就被充分利用起来，这就是<strong>事件驱动</strong>，所谓的reactor模式。</p>
        </li>
      </ol>
    </blockquote>

    <p><strong>这里“多路”指的是多个网络连接，“复用”指的是复用同一个线程。</strong> 采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络 IO 的时间消耗），且 Redis 在内存中操作数据的速度非常快，也就是说内存内的操作不会成为影响Redis性能的瓶颈，主要由以上几点造就了 Redis 具有很高的吞吐量。</p>
  </li>
  <li>
    <p>单线程的redis如何利用多核cpu机器？</p>

    <blockquote>
      <p><a href="https://blog.csdn.net/xlgen157387/article/details/79470556">https://blog.csdn.net/xlgen157387/article/details/79470556</a></p>
    </blockquote>

    <p>我们首先要明白，上边的种种分析，都是为了营造一个Redis很快的氛围！官方FAQ表示，因为Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了（毕竟采用多线程会有很多麻烦！）。</p>

    <p>但是，我们使用单线程的方式是无法发挥多核CPU 性能，不过我们可以通过在单机开多个Redis 实例来完善！</p>
  </li>
  <li>
    <p>redis的缓存淘汰策略？</p>

    <blockquote>
      <p><a href="https://blog.csdn.net/yangtuogege/article/details/77970896">https://blog.csdn.net/yangtuogege/article/details/77970896</a></p>
    </blockquote>

    <p>redis 提供 6种数据淘汰策略：</p>

    <ul>
      <li>volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰</li>
      <li>volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰</li>
      <li>volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰</li>
      <li>allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰</li>
      <li>allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰</li>
      <li>no-enviction（驱逐）：禁止驱逐数据</li>
    </ul>

    <p>上面提到的LRU（Least Recently Used）策略，实际上Redis实现的LRU并不是可靠的LRU，也就是名义上我们使用LRU算法淘汰键，但是实际上被淘汰的键并不一定是真正的最久没用的， 这里涉及到一个权衡的问题，如果需要在全部键空间内搜索最优解，则必然会增加系统的开销，Redis是单线程的， 也就是同一个实例在每一个时刻只能服务于一个客户端，所以耗时的操作一定要谨慎 。为了在一定成本内实现相对的LRU， 早期的Redis版本是基于采样的LRU，也就是放弃全部键空间内搜索解改为采样空间搜索最优解。自从Redis3.0版本之后， Redis作者对于基于采样的LRU进行了一些优化，目的是在一定的成本内让结果更靠近真实的LRU。</p>

    <p><strong>策略规则</strong></p>

    <p>如果数据呈现幂律分布，也就是一部分数据访问频率高，一部分数据访问频率低，则使用allkeys-lru</p>

    <p>如果数据呈现平等分布，也就是所有的数据访问频率都相同，则使用allkeys-random</p>

    <p>volatile-lru策略和volatile-random策略适合我们将一个Redis实例既应用于缓存和又应用于持久化存储的时候，然而我们也可以通过使用两个Redis实例来达到相同的效果，将key设置过期时间实际上会消耗更多的内存，因此我们建议使用allkeys-lru策略从而更有效率的使用内存</p>

    <p><strong>失效的内部实现</strong></p>

    <p>消极方法（passive way），在主键被访问时如果发现它已经失效，那么就删除它</p>

    <p>积极方法（active way），周期性地从设置了失效时间的主键中选择一部分失效的主键删除</p>

    <p>主动删除：当前已用内存超过maxmemory限定时，触发主动清理策略，该策略由启动参数的配置决定</p>
  </li>
  <li>
    <p>redis如何持久化数据？</p>

    <blockquote>
      <p>redis 持久化机制(怎么保证 redis 挂掉之后再重启数据可以进行恢复)</p>
    </blockquote>

    <p>很多时候我们需要持久化数据也就是将内存中的数据写入到硬盘里面，大部分原因是为了之后重用数据（比如重启机器、机器故障之后恢复数据），或者是为了防止系统故障而将数据备份到一个远程位置。</p>

    <p>Redis不同于Memcached的很重一点就是，Redis支持持久化，而且支持两种不同的持久化操作。<strong>Redis的一种持久化方式叫快照（snapshotting，RDB），另一种方式是只追加文件（append-only file,AOF）。</strong> 这两种方法各有千秋，下面我会详细这两种持久化方法是什么，怎么用，如何选择适合自己的持久化方法。</p>

    <p><strong>快照（snapshotting）持久化（RDB）</strong></p>

    <p>Redis可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis主从结构，主要用来提高Redis性能），还可以将快照留在原地以便重启服务器的时候使用。</p>

    <p>快照持久化是Redis默认采用的持久化方式，在redis.conf配置文件中默认有此下配置：</p>

    <div class="language-conf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="n">save</span> <span class="m">900</span> <span class="m">1</span>           <span class="c">#在900秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发BGSAVE命令创建快照。
</span>
<span class="n">save</span> <span class="m">300</span> <span class="m">10</span>          <span class="c">#在300秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发BGSAVE命令创建快照。
</span>
<span class="n">save</span> <span class="m">60</span> <span class="m">10000</span>        <span class="c">#在60秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发BGSAVE命令创建快照。
</span></pre></td></tr></tbody></table></code></pre></div>    </div>

    <p><strong>AOF（append-only file）持久化</strong></p>

    <p>与快照持久化相比，AOF持久化 的实时性更好，因此已成为主流的持久化方案。默认情况下Redis没有开启AOF（append only file）方式的持久化，可以通过appendonly参数开启：</p>

    <div class="language-conf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">appendonly</span> <span class="n">yes</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>

    <p>开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬盘中的AOF文件。AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的，默认的文件名是appendonly.aof。</p>

    <p>在Redis的配置文件中存在三种不同的 AOF 持久化方式，它们分别是：</p>

    <div class="language-conf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="n">appendfsync</span> <span class="n">always</span>    <span class="c">#每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度
</span><span class="n">appendfsync</span> <span class="n">everysec</span>  <span class="c">#每秒钟同步一次，显示地将多个写命令同步到硬盘
</span><span class="n">appendfsync</span> <span class="n">no</span>        <span class="c">#让操作系统决定何时进行同步
</span></pre></td></tr></tbody></table></code></pre></div>    </div>

    <p>为了兼顾数据和写入性能，用户可以考虑 appendfsync everysec选项 ，让Redis每秒同步一次AOF文件，Redis性能几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。</p>

    <p><strong>Redis 4.0 对于持久化机制的优化</strong></p>

    <p>Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。</p>

    <p>如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分是压缩格式不再是 AOF 格式，可读性较差。</p>

    <p><strong>补充内容：AOF 重写</strong></p>

    <p>AOF重写可以产生一个新的AOF文件，这个新的AOF文件和原有的AOF文件所保存的数据库状态一样，但体积更小。</p>

    <p>AOF重写是一个有歧义的名字，该功能是通过读取数据库中的键值对来实现的，程序无须对现有AOF文件进行任何读入、分析或者写入操作。</p>

    <p>在执行 BGREWRITEAOF 命令时，Redis 服务器会维护一个 AOF 重写缓冲区，该缓冲区会在子进程创建新AOF文件期间，记录服务器执行的所有写命令。当子进程完成创建新AOF文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新AOF文件的末尾，使得新旧两个AOF文件所保存的数据库状态一致。最后，服务器用新的AOF文件替换旧的AOF文件，以此来完成AOF文件重写操作。</p>
  </li>
  <li>
    <p>redis有哪几种数据结构？</p>

    <blockquote>
      <p>redis 常见数据结构以及使用场景分析</p>
    </blockquote>

    <p>1.String</p>

    <blockquote>
      <p>常用命令: set,get,decr,incr,mget 等。</p>
    </blockquote>

    <p>String数据结构是简单的key-value类型，value其实不仅可以是String，也可以是数字。 常规key-value缓存应用； 常规计数：微博数，粉丝数等。</p>

    <p>2.Hash</p>

    <blockquote>
      <p>常用命令： hget,hset,hgetall 等。</p>
    </blockquote>

    <p>hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象，后续操作的时候，你可以直接仅仅修改这个对象中的某个字段的值。 比如我们可以 hash 数据结构来存储用户信息，商品信息等等。比如下面我就用 hash 类型存放了我本人的一些信息：</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre>key=JavaUser293847
value={
  “id”: 1,
  “name”: “SnailClimb”,
  “age”: 22,
  “location”: “Wuhan, Hubei”
}
</pre></td></tr></tbody></table></code></pre></div>    </div>

    <p>3.List</p>

    <blockquote>
      <p>常用命令: lpush,rpush,lpop,rpop,lrange等</p>
    </blockquote>

    <p>list 就是链表，Redis list 的应用场景非常多，也是Redis最重要的数据结构之一，比如微博的关注列表，粉丝列表，消息列表等功能都可以用Redis的 list 结构来实现。</p>

    <p>Redis list 的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销。</p>

    <p>另外可以通过 lrange 命令，就是从某个元素开始读取多少个元素，可以基于 list 实现分页查询，这个很棒的一个功能，基于 redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西（一页一页的往下走），性能高。</p>

    <p>4.Set</p>

    <blockquote>
      <p>常用命令： sadd,spop,smembers,sunion 等</p>
    </blockquote>

    <p>set 对外提供的功能与list类似是一个列表的功能，特殊之处在于 set 是可以自动排重的。</p>

    <p>当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在一个set集合内的重要接口，这个也是list所不能提供的。可以基于 set 轻易实现交集、并集、差集的操作。</p>

    <p>比如：在微博应用中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。Redis可以非常方便的实现如共同关注、共同粉丝、共同喜好等功能。这个过程也就是求交集的过程，具体命令如下：</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>sinterstore key1 key2 key3     将交集存在key1内
</pre></td></tr></tbody></table></code></pre></div>    </div>

    <p>5.Sorted Set</p>

    <blockquote>
      <p>常用命令： zadd,zrange,zrem,zcard等</p>
    </blockquote>

    <p>和set相比，sorted set增加了一个权重参数score，使得集合中的元素能够按score进行有序排列。</p>

    <p>举例： 在直播系统中，实时排行信息包含直播间在线用户列表，各种礼物排行榜，弹幕消息（可以理解为按消息维度的消息排行榜）等信息，适合使用 Redis 中的 Sorted Set 结构进行存储。</p>
  </li>
  <li>
    <p>redis集群有哪几种形式？</p>

    <blockquote>
      <p><a href="https://blog.csdn.net/wy0123/article/details/79583506">https://blog.csdn.net/wy0123/article/details/79583506</a></p>
    </blockquote>

    <p>现在越来越多的项目都会利用到redis，多实例redis服务比单实例要复杂的多，这里面涉及到定位、容错、扩容等技术问题。我们常用sharding技术来对此进行管理，其集群模式主要有以下几种方式：</p>

    <ol>
      <li>主从复制</li>
      <li>哨兵模式</li>
      <li>Redis官方 Cluster集群模式（服务端sharding）</li>
      <li>Jedis sharding集群（客户端sharding）</li>
      <li>利用中间件代理</li>
    </ol>

    <ul>
      <li>
        <p><strong>主从复制（Master-Slave Replication）</strong></p>

        <p>实现主从复制（Master-Slave Replication）的工作原理：Slave从节点服务启动并连接到Master之后，它将主动发送一个SYNC命令。Master服务主节点收到同步命令后将启动后台存盘进程，同时收集所有接收到的用于修改数据集的命令，在后台进程执行完毕后，Master将传送整个数据库文件到Slave，以完成一次完全同步。而Slave从节点服务在接收到数据库文件数据之后将其存盘并加载到内存中。此后，Master主节点继续将所有已经收集到的修改命令，和新的修改命令依次传送给Slaves，Slave将在本次执行这些数据修改命令，从而达到最终的数据同步。</p>

        <p>如果Master和Slave之间的链接出现断连现象，Slave可以自动重连Master，但是在连接成功之后，一次完全同步将被自动执行。</p>

        <p><strong>主从模式的优缺点</strong></p>

        <p>优点：</p>

        <ul>
          <li>同一个Master可以同步多个Slaves。</li>
          <li>Slave同样可以接受其它Slaves的连接和同步请求，这样可以有效的分载Master的同步压力。因此我们可以将Redis的Replication架构视为图结构。</li>
          <li>Master Server是以非阻塞的方式为Slaves提供服务。所以在Master-Slave同步期间，客户端仍然可以提交查询或修改请求。</li>
          <li>Slave Server同样是以非阻塞的方式完成数据同步。在同步期间，如果有客户端提交查询请求，Redis则返回同步之前的数据</li>
          <li>为了分载Master的读操作压力，Slave服务器可以为客户端提供只读操作的服务，写服务仍然必须由Master来完成。即便如此，系统的伸缩性还是得到了很大的提高。</li>
          <li>Master可以将数据保存操作交给Slaves完成，从而避免了在Master中要有独立的进程来完成此操作。</li>
          <li>支持主从复制，主机会自动将数据同步到从机，可以进行读写分离。</li>
        </ul>

        <p>缺点：</p>

        <ul>
          <li>Redis不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复。</li>
          <li>主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性。</li>
          <li>Redis的主从复制采用全量复制，复制过程中主机会fork出一个子进程对内存做一份快照，并将子进程的内存快照保存为文件发送给从机，这一过程需要确保主机有足够多的空余内存。若快照文件较大，对集群的服务能力会产生较大的影响，而且复制过程是在从机新加入集群或者从机和主机网络断开重连时都会进行，也就是网络波动都会造成主机和从机间的一次全量的数据复制，这对实际的系统运营造成了不小的麻烦。</li>
          <li>Redis较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。</li>
        </ul>

        <p>其实redis的主从模式很简单，在实际的生产环境中是很少使用的，我也不建议在实际的生产环境中使用主从模式来提供系统的高可用性，之所以不建议使用都是由它的缺点造成的，在数据量非常大的情况，或者对系统的高可用性要求很高的情况下，主从模式也是不稳定的。</p>
      </li>
      <li>
        <p><strong>哨兵模式</strong></p>

        <p>该模式是从Redis的2.6版本开始提供的，但是当时这个版本的模式是不稳定的，直到Redis的2.8版本以后，这个哨兵模式才稳定下来，无论是主从模式，还是哨兵模式，这两个模式都有一个问题，不能水平扩容，并且这两个模式的高可用特性都会受到Master主节点内存的限制。</p>

        <p>Sentinel(哨兵)进程是用于监控redis集群中Master主服务器工作的状态，在Master主服务器发生故障的时候，可以实现Master和Slave服务器的切换，保证系统的高可用。</p>

        <p><strong>Sentinel（哨兵）进程的作用</strong></p>

        <ol>
          <li>监控(Monitoring): 哨兵(sentinel) 会不断地检查你的Master和Slave是否运作正常。</li>
          <li>提醒(Notification)：当被监控的某个Redis节点出现问题时, 哨兵(sentinel) 可以通过 API 向管理员或者其他应用程序发送通知。</li>
          <li>自动故障迁移(Automatic failover)：当一个Master不能正常工作时，哨兵(sentinel) 会开始一次自动故障迁移操作，它会将失效Master的其中一个Slave升级为新的Master, 并让失效Master的其他Slave改为复制新的Master；当客户端试图连接失效的Master时，集群也会向客户端返回新Master的地址，使得集群可以使用现在的Master替换失效Master。Master和Slave服务器切换后，Master的redis.conf、Slave的redis.conf和sentinel.conf的配置文件的内容都会发生相应的改变，即，Master主服务器的redis.conf配置文件中会多一行slaveof的配置，sentinel.conf的监控目标会随之调换。</li>
        </ol>

        <p><strong>Sentinel（哨兵）进程的工作方式</strong></p>

        <ol>
          <li>每个Sentinel（哨兵）进程以每秒钟一次的频率向整个集群中的Master主服务器，Slave从服务器以及其他Sentinel（哨兵）进程发送一个 PING 命令。</li>
          <li>如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 则这个实例会被 Sentinel（哨兵）进程标记为主观下线（SDOWN）</li>
          <li>如果一个Master主服务器被标记为主观下线（SDOWN），则正在监视这个Master主服务器的所有 Sentinel（哨兵）进程要以每秒一次的频率确认Master主服务器的确进入了主观下线状态</li>
          <li>当有足够数量的 Sentinel（哨兵）进程（大于等于配置文件指定的值）在指定的时间范围内确认Master主服务器进入了主观下线状态（SDOWN）， 则Master主服务器会被标记为客观下线（ODOWN）</li>
          <li>在一般情况下， 每个 Sentinel（哨兵）进程会以每 10 秒一次的频率向集群中的所有Master主服务器、Slave从服务器发送 INFO 命令。</li>
          <li>当Master主服务器被 Sentinel（哨兵）进程标记为客观下线（ODOWN）时，Sentinel（哨兵）进程向下线的 Master主服务器的所有 Slave从服务器发送 INFO 命令的频率会从 10 秒一次改为每秒一次。</li>
          <li>若没有足够数量的 Sentinel（哨兵）进程同意 Master主服务器下线， Master主服务器的客观下线状态就会被移除。若 Master主服务器重新向 Sentinel（哨兵）进程发送 PING 命令返回有效回复，Master主服务器的主观下线状态就会被移除。</li>
        </ol>

        <p><strong>哨兵模式的优缺点</strong></p>

        <p>优点:</p>

        <ul>
          <li>哨兵集群模式是基于主从模式的，所有主从的优点，哨兵模式同样具有。</li>
          <li>主从可以切换，故障可以转移，系统可用性更好。</li>
          <li>哨兵模式是主从模式的升级，系统更健壮，可用性更高。</li>
        </ul>

        <p>缺点:</p>

        <ul>
          <li>Redis较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。</li>
          <li>配置复杂</li>
        </ul>
      </li>
      <li>
        <p><strong>Redis官方 Cluster集群模式</strong></p>

        <p>Redis Cluster是一种服务器Sharding技术，3.0版本开始正式提供。</p>

        <p><img src="https://img-blog.csdn.net/20180319140642333?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3d5MDEyMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" /></p>

        <p>在这个图中，每一个蓝色的圈都代表着一个redis的服务器节点。它们任何两个节点之间都是相互连通的。客户端可以与任何一个节点相连接，然后就可以访问集群中的任何一个节点。对其进行存取和其他操作。</p>

        <p><strong>Redis集群数据分片</strong></p>

        <p>在redis的每一个节点上，都有这么两个东西，一个是插槽（slot）可以理解为是一个可以存储两个数值的一个变量这个变量的取值范围是：0-16383。还有一个就是cluster我个人把这个cluster理解为是一个集群管理的插件。当我们的存取的key到达的时候，redis会根据crc16的算法得出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，通过这个值，去找到对应的插槽所对应的节点，然后直接自动跳转到这个对应的节点上进行存取操作。</p>

        <p><img src="https://img-blog.csdn.net/20180319141211304?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3d5MDEyMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" /></p>

        <p>还有就是因为如果集群的话，是有好多个redis一起工作的，那么，就需要这个集群不是那么容易挂掉，所以呢，理论上就应该给集群中的每个节点至少一个备用的redis服务。这个备用的redis称为从节点（slave）。那么这个集群是如何判断是否有某个节点挂掉了呢？</p>

        <p>首先要说的是，每一个节点都存有这个集群所有主节点以及从节点的信息。</p>

        <p>它们之间通过互相的ping-pong判断是否节点可以连接上。如果有一半以上的节点去ping一个节点的时候没有回应，集群就认为这个节点宕机了，然后去连接它的备用节点。如果某个节点和所有从节点全部挂掉，我们集群就进入fail状态。还有就是如果有一半以上的主节点宕机，那么我们集群同样进入fail状态。这就是我们的redis的投票机制，具体原理如下图所示：</p>

        <p><img src="https://img-blog.csdn.net/20180319141325659?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3d5MDEyMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" /></p>

        <p>Redis 3.0的集群方案有以下两个问题。</p>

        <ol>
          <li>一个Redis实例具备了“数据存储”和“路由重定向”，完全去中心化的设计。这带来的好处是部署非常简单，直接部署Redis就行，不像Codis有那么多的组件和依赖。但带来的问题是很难对业务进行无痛的升级，如果哪天Redis集群出了什么严重的Bug，就只能回滚整个Redis集群。</li>
          <li>对协议进行了较大的修改，对应的Redis客户端也需要升级。升级Redis客户端后谁能确保没有Bug？而且对于线上已经大规模运行的业务，升级代码中的Redis客户端也是一个很麻烦的事情。</li>
        </ol>

        <p>Redis Cluster是Redis 3.0以后才正式推出，时间较晚，目前能证明在大规模生产环境下成功的案例还不是很多，需要时间检验。</p>
      </li>
      <li>
        <p><strong>Jedis sharding集群</strong></p>

        <p>Redis Sharding可以说是在Redis cluster出来之前业界普遍的采用方式，其主要思想是采用hash算法将存储数据的key进行hash散列，这样特定的key会被定为到特定的节点上。</p>

        <p><img src="https://img-blog.csdn.net/20180319143624440?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3d5MDEyMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" /></p>

        <p>庆幸的是，Java Redis客户端驱动Jedis已支持Redis Sharding功能，即ShardedJedis以及结合缓存池的ShardedJedisPool</p>

        <p>Jedis的Redis Sharding实现具有如下特点：</p>

        <ol>
          <li>采用一致性哈希算法，将key和节点name同时hashing，然后进行映射匹配，采用的算法是MURMUR_HASH。采用一致性哈希而不是采用简单类似哈希求模映射的主要原因是当增加或减少节点时，不会产生由于重新匹配造成的rehashing。一致性哈希只影响相邻节点key分配，影响量小。</li>
          <li>为了避免一致性哈希只影响相邻节点造成节点分配压力，ShardedJedis会对每个Redis节点根据名字(没有，Jedis会赋予缺省名字)会虚拟化出160个虚拟节点进行散列。根据权重weight，也可虚拟化出160倍数的虚拟节点。用虚拟节点做映射匹配，可以在增加或减少Redis节点时，key在各Redis节点移动再分配更均匀，而不是只有相邻节点受影响。</li>
          <li>ShardedJedis支持keyTagPattern模式，即抽取key的一部分keyTag做sharding，这样通过合理命名key，可以将一组相关联的key放入同一个Redis节点，这在避免跨节点访问相关数据时很重要。</li>
        </ol>

        <p>当然，Redis Sharding这种轻量灵活方式必然在集群其它能力方面做出妥协。比如扩容，当想要增加Redis节点时，尽管采用一致性哈希，毕竟还是会有key匹配不到而丢失，这时需要键值迁移。</p>

        <p>作为轻量级客户端sharding，处理Redis键值迁移是不现实的，这就要求应用层面允许Redis中数据丢失或从后端数据库重新加载数据。但有些时候，击穿缓存层，直接访问数据库层，会对系统访问造成很大压力。</p>
      </li>
      <li>
        <p><strong>利用中间件代理</strong></p>

        <p>中间件的作用是将我们需要存入redis中的数据的key通过一套算法计算得出一个值。然后根据这个值找到对应的redis节点，将这些数据存在这个redis的节点中。</p>

        <p>常用的中间件有这几种</p>

        <ul>
          <li>Twemproxy</li>
          <li>Codis</li>
          <li>nginx</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>有海量key和value都比较小的数据，在redis中如何存储才更省内存？</p>

    <blockquote>
      <p><a href="https://zzyongx.github.io/blogs/redis-memory-optimization-when-store-small-data.html">https://zzyongx.github.io/blogs/redis-memory-optimization-when-store-small-data.html</a></p>
    </blockquote>

    <ul>
      <li>
        <p>使用二进制存储：<strong>32位数转成16位数存储</strong></p>
      </li>
      <li>
        <p>使用SET和HSET混合的数据组织方式</p>

        <p>先看两个很有意思的配置，是专门为小Hash做准备（使用HSET），当Hash中的条目小于512，并且每个value小于64个字节时，Redis内部采用特殊的编码方式，可以使内存平均节省5倍。</p>

        <blockquote>
          <p>https://www.jianshu.com/p/8764a3e7b090</p>
        </blockquote>

        <blockquote>
          <p>如果该 Map 的成员数比较少，则会采用类似一维线性的紧凑格式来存储该 Map，即省去了大量指针的内存开销</p>
        </blockquote>

        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>hash-max-ziplist-entries 512
hash-max-ziplist-value 64
</pre></td></tr></tbody></table></code></pre></div>        </div>

        <p><strong>我们可以把key-value的结构拆解成key-smallhash这样的结构来降低内存的使用</strong></p>
      </li>
    </ul>
  </li>
  <li>
    <p>如何保证redis和DB中的数据一致性？</p>

    <blockquote>
      <p>如何保证缓存与数据库双写时的数据一致性?</p>
    </blockquote>

    <blockquote>
      <p>一般情况下我们都是这样使用缓存的：先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。这种方式很明显会存在缓存和数据库的数据不一致的情况。</p>
    </blockquote>

    <p>你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？</p>

    <p>一般来说，就是如果你的系统不是严格要求缓存+数据库必须一致性的话，缓存可以稍微的跟数据库偶尔有不一致的情况，最好不要做这个方案，读请求和写请求串行化，串到一个内存队列里去，这样就可以保证一定不会出现不一致的情况</p>

    <p>串行化之后，就会导致系统的吞吐量会大幅度的降低，用比正常情况下多几倍的机器去支撑线上的一个请求。</p>

    <blockquote>
      <p><a href="https://blog.csdn.net/qq_27384769/article/details/79499373">https://blog.csdn.net/qq_27384769/article/details/79499373</a></p>
    </blockquote>

    <p><strong>数据一致性的原因</strong></p>

    <p>写流程：</p>

    <p>1）先淘汰cache</p>

    <p>2）再写db</p>

    <p>读流程：</p>

    <p>1）先读cache，如果数据命中hit则返回</p>

    <p>2）如果数据未命中miss则读db</p>

    <p>3）将db中读取出来的数据入缓存</p>

    <p>什么情况下可能出现缓存和数据库中数据不一致呢？</p>

    <p><img src="https://github.com/csy512889371/learnDoc/raw/master/image/2018/redis/3.png" alt="img" /></p>

    <p>在分布式环境下，数据的读写都是并发的，上游有多个应用，通过一个服务的多个部署（为了保证可用性，一定是部署多份的），对同一个数据进行读写，在数据库层面并发的读写并不能保证完成顺序，也就是说后发出的读请求很可能先完成（读出脏数据）：</p>

    <p>a）发生了写请求A，A的第一步淘汰了cache（如上图中的1）</p>

    <p>b）A的第二步写数据库，发出修改请求（如上图中的2）</p>

    <p>c）发生了读请求B，B的第一步读取cache，发现cache中是空的（如上图中的步骤3）</p>

    <p>d）B的第二步读取数据库，发出读取请求，此时A的第二步写数据还没完成，读出了一个脏数据放入cache（如上图中的步骤4）</p>

    <p>即在数据库层面，后发出的请求4比先发出的请求2先完成了，读出了脏数据，脏数据又入了缓存，缓存与数据库中的数据不一致出现了</p>

    <p><strong>问题解决思路</strong></p>

    <p>能否做到先发出的请求一定先执行完成呢？常见的思路是“串行化”</p>

    <p><img src="https://github.com/csy512889371/learnDoc/raw/master/image/2018/redis/4.png" alt="img" /></p>

    <p>上图是一个service服务的上下游及服务内部详细展开，细节如下：</p>

    <p>1）service的上游是多个业务应用，上游发起请求对同一个数据并发的进行读写操作，上例中并发进行了一个uid=1的余额修改（写）操作与uid=1的余额查询（读）操作</p>

    <p>2）service的下游是数据库DB，假设只读写一个DB</p>

    <p>3）中间是服务层service，它又分为了这么几个部分</p>

    <p>3.1）最上层是任务队列</p>

    <p>3.2）中间是工作线程，每个工作线程完成实际的工作任务，典型的工作任务是通过数据库连接池读写数据库</p>

    <p>3.3）最下层是数据库连接池，所有的SQL语句都是通过数据库连接池发往数据库去执行的</p>
  </li>
  <li>
    <p>如何解决缓存穿透和缓存雪崩？</p>

    <blockquote>
      <p>缓存雪崩和缓存穿透问题解决方案</p>
    </blockquote>

    <p><strong>缓存雪崩</strong></p>

    <p>什么是缓存雪崩？</p>

    <p>缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。</p>

    <p>解决方法：</p>

    <ul>
      <li>事前：尽量保证整个 redis 集群的高可用性，发现机器宕机尽快补上。选择合适的内存淘汰策略。</li>
      <li>事中：本地ehcache缓存 + hystrix限流&amp;降级，避免MySQL崩掉</li>
      <li>事后：利用 redis 持久化机制保存的数据尽快恢复缓存</li>
    </ul>

    <p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/18-9-25/6078367.jpg" alt="img" /></p>

    <p><strong>缓存穿透</strong></p>

    <p>什么是缓存穿透？</p>

    <p>缓存穿透说简单点就是大量请求的 key 根本不存在于缓存中，导致请求直接到了数据库上，根本没有经过缓存这一层。举个例子：某个黑客故意制造我们缓存中不存在的 key 发起大量请求，导致大量请求落到数据库。下面用图片展示一下(这两张图片不是我画的，为了省事直接在网上找的，这里说明一下)：</p>

    <p>正常缓存处理流程：</p>

    <p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/%E6%AD%A3%E5%B8%B8%E7%BC%93%E5%AD%98%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B-redis.png" alt="img" /></p>

    <p>缓存穿透情况处理流程：</p>

    <p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B-redis.png" alt="img" /></p>

    <p>一般MySQL 默认的最大连接数在 150 左右，这个可以通过 <code class="highlighter-rouge">show variables like '%max_connections%';</code>命令来查看。最大连接数一个还只是一个指标，cpu，内存，磁盘，网络等物理条件都是其运行指标，这些指标都会限制其并发能力！所以，一般 3000 个并发请求就能打死大部分数据库了。</p>

    <p>有哪些解决办法？</p>

    <p>最基本的就是首先做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端。比如查询的数据库 id 不能小于 0、传入的邮箱格式不对的时候直接返回错误消息给客户端等等。</p>

    <p>1）缓存无效 key : 如果缓存和数据库都查不到某个 key 的数据就写一个到 redis 中去并设置过期时间，具体命令如下：SET key value EX 10086。这种方式可以解决请求的 key 变化不频繁的情况，如何黑客恶意攻击，每次构建的不同的请求key，会导致 redis 中缓存大量无效的 key 。很明显，这种方案并不能从根本上解决此问题。如果非要用这种方式来解决穿透问题的话，尽量将无效的 key 的过期时间设置短一点比如 1 分钟。</p>

    <p>另外，这里多说一嘴，一般情况下我们是这样设计 key 的： 表名:列名:主键名:主键值。</p>

    <p>如果用 Java 代码展示的话，差不多是下面这样的：</p>

    <div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="rouge-code"><pre><span class="kd">public</span> <span class="nc">Object</span> <span class="nf">getObjectInclNullById</span><span class="o">(</span><span class="nc">Integer</span> <span class="n">id</span><span class="o">)</span> <span class="o">{</span>
  <span class="c1">// 从缓存中获取数据</span>
  <span class="nc">Object</span> <span class="n">cacheValue</span> <span class="o">=</span> <span class="n">cache</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">id</span><span class="o">);</span>
  <span class="c1">// 缓存为空</span>
  <span class="k">if</span> <span class="o">(</span><span class="n">cacheValue</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
      <span class="c1">// 从数据库中获取</span>
      <span class="nc">Object</span> <span class="n">storageValue</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">key</span><span class="o">);</span>
      <span class="c1">// 缓存空对象</span>
      <span class="n">cache</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">key</span><span class="o">,</span> <span class="n">storageValue</span><span class="o">);</span>
      <span class="c1">// 如果存储数据为空，需要设置一个过期时间(300秒)</span>
      <span class="k">if</span> <span class="o">(</span><span class="n">storageValue</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
          <span class="c1">// 必须设置过期时间，否则有被攻击的风险</span>
          <span class="n">cache</span><span class="o">.</span><span class="na">expire</span><span class="o">(</span><span class="n">key</span><span class="o">,</span> <span class="mi">60</span> <span class="o">*</span> <span class="mi">5</span><span class="o">);</span>
      <span class="o">}</span>
      <span class="k">return</span> <span class="n">storageValue</span><span class="o">;</span>
  <span class="o">}</span>
  <span class="k">return</span> <span class="n">cacheValue</span><span class="o">;</span>
<span class="o">}</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>

    <p>2）布隆过滤器：布隆过滤器是一个非常神奇的数据结构，通过它我们可以非常方便地判断一个给定数据是否存在与海量数据中。我们需要的就是判断 key 是否合法，有没有感觉布隆过滤器就是我们想要找的那个“人”。具体是这样做的：把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，我会先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话才会走下面的流程。总结一下就是下面这张图(这张图片不是我画的，为了省事直接在网上找的)：</p>

    <p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8-%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F-redis.png" alt="img" /></p>
  </li>
  <li>
    <p>如何用redis实现分布式锁？</p>

    <p><strong>什么是 RedLock</strong></p>

    <p>Redis 官方站这篇文章提出了一种权威的基于 Redis 实现分布式锁的方式名叫 Redlock，此种方式比原先的单节点的方法更安全。它可以保证以下特性：</p>

    <ol>
      <li>安全特性：互斥访问，即永远只有一个 client 能拿到锁</li>
      <li>避免死锁：最终 client 都可能拿到锁，不会出现死锁的情况，即使原本锁住某资源的 client crash 了或者出现了网络分区</li>
      <li>容错性：只要大部分 Redis 节点存活就可以正常提供服务</li>
    </ol>

    <p><strong>怎么在单节点上实现分布式锁</strong></p>

    <blockquote>
      <p>SET resource_name my_random_value NX PX 30000</p>
    </blockquote>

    <p>主要依靠上述命令，该命令<strong>仅当 Key 不存在时（NX保证）set 值</strong>，并且<strong>设置过期时间</strong> 3000ms （PX保证），<strong>值 my_random_value 必须是所有 client 和所有锁请求发生期间唯一的</strong>，释放锁的逻辑是：</p>

    <div class="language-lua highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="k">if</span> <span class="n">redis</span><span class="p">.</span><span class="n">call</span><span class="p">(</span><span class="s2">"get"</span><span class="p">,</span><span class="n">KEYS</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">ARGV</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">then</span>
    <span class="k">return</span> <span class="n">redis</span><span class="p">.</span><span class="n">call</span><span class="p">(</span><span class="s2">"del"</span><span class="p">,</span><span class="n">KEYS</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="k">else</span>
    <span class="k">return</span> <span class="mi">0</span>
<span class="k">end</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>

    <p>上述实现可以避免释放另一个client创建的锁，如果只有 del 命令的话，那么如果 client1 拿到 lock1 之后因为某些操作阻塞了很长时间，此时 Redis 端 lock1 已经过期了并且已经被重新分配给了 client2，那么 client1 此时再去释放这把锁就会造成 client2 原本获取到的锁被 client1 无故释放了，但现在为每个 client 分配一个 unique 的 string 值可以避免这个问题。至于如何去生成这个 unique string，方法很多随意选择一种就行了。</p>

    <p><strong>Redlock 算法</strong></p>

    <p>算法很易懂，起 5 个 master 节点，分布在不同的机房尽量保证可用性。为了获得锁，client 会进行如下操作：</p>

    <ol>
      <li>得到当前的时间，微秒单位</li>
      <li>尝试顺序地在 5 个实例上申请锁，当然需要使用相同的 key 和 random value，这里一个 client 需要合理设置与 master 节点沟通的 timeout 大小，避免长时间和一个 fail 了的节点浪费时间</li>
      <li>当 client 在大于等于 3 个 master 上成功申请到锁的时候，且它会计算申请锁消耗了多少时间，这部分消耗的时间采用获得锁的当下时间减去第一步获得的时间戳得到，如果锁的持续时长（lock validity time）比流逝的时间多的话，那么锁就真正获取到了。</li>
      <li>如果锁申请到了，那么锁真正的 lock validity time 应该是 origin（lock validity time） - 申请锁期间流逝的时间</li>
      <li>如果 client 申请锁失败了，那么它就会在少部分申请成功锁的 master 节点上执行释放锁的操作，重置状态</li>
    </ol>

    <p><strong>失败重试</strong></p>

    <p>如果一个 client 申请锁失败了，那么它需要稍等一会在重试避免多个 client 同时申请锁的情况，最好的情况是一个 client 需要几乎同时向 5 个 master 发起锁申请。另外就是如果 client 申请锁失败了它需要尽快在它曾经申请到锁的 master 上执行 unlock 操作，便于其他 client 获得这把锁，避免这些锁过期造成的时间浪费，当然如果这时候网络分区使得 client 无法联系上这些 master，那么这种浪费就是不得不付出的代价了。</p>

    <p><strong>放锁</strong></p>

    <p>放锁操作很简单，就是依次释放所有节点上的锁就行了</p>

    <p><strong>性能、崩溃恢复和 fsync</strong></p>

    <p>如果我们的节点没有持久化机制，client 从 5 个 master 中的 3 个处获得了锁，然后其中一个重启了，这时注意 <strong>整个环境中又出现了 3 个 master 可供另一个 client 申请同一把锁！</strong> 违反了互斥性。如果我们开启了 AOF 持久化那么情况会稍微好转一些，因为 Redis 的过期机制是语义层面实现的，所以在 server 挂了的时候时间依旧在流逝，重启之后锁状态不会受到污染。但是考虑断电之后呢，AOF部分命令没来得及刷回磁盘直接丢失了，除非我们配置刷回策略为 fsnyc = always，但这会损伤性能。解决这个问题的方法是，当一个节点重启之后，我们规定在 max TTL 期间它是不可用的，这样它就不会干扰原本已经申请到的锁，等到它 crash 前的那部分锁都过期了，环境不存在历史锁了，那么再把这个节点加进来正常工作。</p>
  </li>
</ul>


                <hr style="visibility: hidden;">
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2020/03/05/concurrency/" data-toggle="tooltip" data-placement="top" title="并发">
                        Previous<br>
                        <span>并发</span>
                        </a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2020/03/05/designpattern/" data-toggle="tooltip" data-placement="top" title="设计模式">
                        Next<br>
                        <span>设计模式</span>
                        </a>
                    </li>
                    
                </ul>
                <hr style="visibility: hidden;">

                

                
            </div>  

    <!-- Side Catalog Container -->
        
            <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                <div class="side-catalog">
                    <hr class="hidden-sm hidden-xs">
                    <h5>
                        <a class="catalog-toggle" href="#">CATALOG</a>
                    </h5>
                    <ul class="catalog-body"></ul>
                </div>
            </div>
        

    <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                


<section>
    
        <hr class="hidden-sm hidden-xs">
    
    <h5><a href="/archive/">FEATURED TAGS</a></h5>
    <div class="tags">
        
        
        
        </a>
        
        
                <a data-sort="0005" 
                    href="/archive/?tag=%E5%AD%A6%E4%B9%A0"
                    title="学习"
                    rel="17">学习</a>
        
                <a data-sort="0018" 
                    href="/archive/?tag=%E7%94%9F%E6%B4%BB"
                    title="生活"
                    rel="4">生活
    </div>
</section>


                <!-- Friends Blog -->
                
<hr>
<h5>FRIENDS</h5>
<ul class="list-inline">
  
  <li><a href="https://github.com/thisaway">Tob</a></li>
  
  <li><a href="https://github.com/MMochii">Mochiii</a></li>
  
</ul>

            </div>
        </div>
    </div>
</article>

<!-- add support for mathjax by voleking-->

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    SVG: {
      scale: 90
    },
    tex2jax: {
      inlineMath: [ ['$','$'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    }
  });
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG">
</script>









<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'right',
          // icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <!-- SNS Link -->
                


<ul class="list-inline text-center">


  
  
  <!-- add Douban by yycg-->
  
  <li>
      <a target="_blank" href="https://www.douban.com/people/154702906">
          <span class="fa-stack fa-lg">
              <i class="fa fa-circle fa-stack-2x"></i>
              <i class="fa  fa-stack-1x fa-inverse">豆</i>
          </span>
      </a>
  </li>
  
  
  
  <li>
    <a target="_blank" href="http://weibo.com/5674063334">
      <span class="fa-stack fa-lg">
        <i class="fa fa-circle fa-stack-2x"></i>
        <i class="fa fa-weibo fa-stack-1x fa-inverse"></i>
      </span>
    </a>
  </li>
  
  
  
  <li>
    <a target="_blank" href="https://github.com/yycg">
      <span class="fa-stack fa-lg">
        <i class="fa fa-circle fa-stack-2x"></i>
        <i class="fa fa-github fa-stack-1x fa-inverse"></i>
      </span>
    </a>
  </li>
  
  
</ul>

                <p class="copyright text-muted">
                    Copyright &copy; 盈盈冲哥的博客 2020
                    <br>
                    Powered by <a href="http://huangxuan.me">Hux Blog</a> |
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=huxpro&repo=huxpro.github.io&type=star&count=true" >
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<!-- Currently, only navbar scroll-down effect at desktop still depends on this -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js "></script>

<!-- Service Worker -->

<script src="/js/snackbar.js "></script>
<script src="/js/sw-registration.js "></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!--
     Because of the native support for backtick-style fenced code blocks
     right within the Markdown is landed in Github Pages,
     From V1.6, There is no need for Highlight.js,
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/
     - https://github.com/jneen/rouge/wiki/list-of-supported-languages-and-lexers
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->





<!--fastClick.js -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->

<script>
    // dynamic User by Hux
    var _gaId = 'UA-49627206-1';
    var _gaDomain = 'huangxuan.me';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>



<!-- Baidu Tongji -->



<!-- Side Catalog -->

<script type="text/javascript">
    function generateCatalog (selector) {

        // interop with multilangual 
        if ('' == 'true') {
            _containerSelector = 'div.post-container.active'
        } else {
            _containerSelector = 'div.post-container'
        }

        // init
        var P = $(_containerSelector),a,n,t,l,i,c;
        a = P.find('h1,h2,h3,h4,h5,h6');

        // clean
        $(selector).html('')

        // appending
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#"+$(this).prop('id');
            t = $(this).text();
            c = $('<a href="'+i+'" rel="nofollow">'+t+'</a>');
            l = $('<li class="'+n+'_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;
    }

    generateCatalog(".catalog-body");

    // toggle side catalog
    $(".catalog-toggle").click((function(e){
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    /*
     * Doc: https://github.com/davist11/jQuery-One-Page-Nav
     * Fork by Hux to support padding
     */
    async("/js/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>



<!-- Multi-Lingual -->




<!-- Image to hack wechat -->
<img src="/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
